{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adveserial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative Adveserial Networks (GAN) are type of neural network architecture that is originally designed to generate samples which are close to orignal sample space. As the name suggests it is Generative network, which means that there is no supervised learning required. The general structure of GANs is provided in figure below. The figure shows two different networks present in the framework. They are commonly named Generator Network and Discriminator Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two networks could be categorized as adversaries of each other. The basic purpose of the Generator Network is to learn the data distribution which is almost equivalent to original data distribution. The Generator Network starts with the noisy distribution and starts to learn from it. The network learns from the feedback provided by the Discriminator Network. The Discriminator could generally be any function that is differentiable. The network is trained on the sample latent data that have dimensions less than or equal to that of original data. Using this strategy allows the Generator to learn so generic features and represent the sample space in less dimesions. This ease the way to create further samples which are not in the original space. Inputs to the generator are randomly sampled from the modelâ€™s prior over the latent variables.\n",
    "\n",
    "The Discriminator Network is provided with two types of data; the original data and the noisy data (i.e: data from Generator Network). Discriminator Network could be thought of as a binary classifier. The task of this binary classifier is to distinguish original data from generated data. It outputs 1 when the classifier thinks that the data is from original distribution and it outputs 0 if the classifier thinks that the data is from Generator Network.\n",
    "\n",
    "Overall framework of GAN could be seen as a two player game where Generator Network is trying to get past the Discriminator Network by making it believe that the generated sample is from the original distibution. On the other hand Discriminator Network tries it best to not be fooled by Generative Network. In Ideal case If both models have sufficient capacity, then the Nash equilibrium of this game corresponds to the G(z) being drawn from the same distribution as the training data, and D(x) = 1/2 for all x.\n",
    "\n",
    "**The GANs have mostly similar to VAEs, the most salient difference is that, if relying on standard backprop, VAEs cannot have discrete variables at the input to the generator, while GANs cannot have discrete variables at the output of the generator. The GAN approximation is subject to the failures of supervised learning: overfitting and underfitting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Cost Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There could be two types of training, for the first part of the training the Discriminator is fed with the original data. The data has half fake and half original data samples. The goal of Descriminator is to identify as much original data samples and output 1. \n",
    "\n",
    "In second part, generator is fed with latent variables from model's prior. The Generator tries to create fake data items and those data items are passed to Discriminator Network. In this part, the goal of Generator is to keep the output near to 1 whereas Discriminator tries to keep output near 0.\n",
    "\n",
    "There are two cost functions for this framework. One for Generative Network and one for Discriminator Network. The commonly used cost function for Discriminator Network is modified cross entrophy, where it is expected that there are two types on inputs.\n",
    "\n",
    "$$C_{D} = -\\frac{1}{2}\\mathbb{E}_{x\\sim p(data)}log(D(x)) -\\frac{1}{2}\\mathbb{E}_{z}log(1- D(G(z)))$$\n",
    "\n",
    "As could be seen from equation above, it maximize the data samples with are from original data and minimize the samples which are generated from Generator Network.\n",
    "\n",
    "By training the discriminator we get the estimated ration of the two models\n",
    "\n",
    "$$\\frac{p_{data}(x)}{p_{model}(x)}$$\n",
    "\n",
    "This is key approximation technqiue that sets GANs apart form variational autoencoders and Boltzmann machines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for Generator Network a simple cost function would be negative of the Discriminator cost function \n",
    "\n",
    "$$C_{G} = - C_{D}$$\n",
    "\n",
    "The advantage of this approch is that we would only need to calculate the value function for adversary game. We can than find minmax function for one cost function and that would optimize the whole framework.\n",
    "\n",
    "$$\\theta_{G} = \\text{argmin}_{\\theta^{(G)}}  \\text{max}_{\\theta^{(D)}}C_{G}$$\n",
    "\n",
    "However, there is also a big disadvantage of this cost function. Imagine a scenerio where discriminator successfully rejects generator samples with high confidence, in this case the generators's gradients vanishes. This could be intuitively seen in the above equation of cost function for discriminator. When D(G(z)) is 0 or near zero than the cost is almost zero, and taking derivative with zero initial value will vanish the gradients before reaching the input layer of neural network.\n",
    "\n",
    "To overcome this problem we use a different cost function for Generator network.\n",
    "\n",
    "$$C_{G} = -\\frac{1}{2}\\mathbb{E}_{z}log(D(G(z)))$$\n",
    "\n",
    "Intuiviely this means that the generator maximizes the logprobability\n",
    "of the discriminator being mistaken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep, Convolutional GAN (DCGAN), is the common architecture used in modern applications. DCGANs were initially used to generate high resolution images. They perform this task in one iteration where as prior to DCGANs, LAPGANs were used which required generation of image multiples in one iteration.\n",
    "\n",
    "The best practicies for training DCGAN can be found on the following side https://github.com/soumith/ganhacks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get packges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D, Conv2DTranspose\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define initial variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input shape\n",
    "img_rows = 28\n",
    "img_cols = 28\n",
    "channels = 1\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "latent_dim = 100\n",
    "\n",
    "optimizer = Adam(0.0002, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper of Radford et al. gives some hints about what is a good DCGAN architecture :\n",
    "\n",
    "* Replace any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator). In other words, when the generator needs to increase the spatial dimension of the representation, it uses transposed convolution with a stride greater than 1\n",
    "\n",
    "* Use Batch Normalization in both the generator (except at the output layer) and the discriminator (except at the input layer)\n",
    "* Remove fully connected hidden layers for deeper architectures\n",
    "* Use ReLU activation in generator for all layers except for the output\n",
    "* Use LeakyReLU activation in the discriminator for all layers\n",
    "\n",
    "For creation of generator, we have used Transposed conv2D layer with batch normalization applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./files/dc-gan-flow.png\" \n",
    "alt=\"IMAGE ALT TEXT HERE\" border=\"10\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildGenerator():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(128 * 28 * 28, activation=\"relu\", input_dim=latent_dim))\n",
    "    model.add(Reshape((28, 28, 128)))\n",
    "    model.add(Conv2DTranspose(128, kernel_size=3, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Conv2DTranspose(64, kernel_size=3, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Conv2DTranspose(channels, kernel_size=3, padding=\"same\"))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    noise = Input(shape=(latent_dim,))\n",
    "    img = model(noise)\n",
    "\n",
    "    return Model(noise, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discriminator is\n",
    "generated using the Conv2D layer, with activation function set to LeakyReLU and batch Normalization is applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDiscriminator():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=img_shape, padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    img = Input(shape=img_shape)\n",
    "    validity = model(img)\n",
    "\n",
    "    return Model(img, validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 4097      \n",
      "=================================================================\n",
      "Total params: 393,729\n",
      "Trainable params: 392,833\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 100352)            10135552  \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 28, 28, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 28, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 28, 28, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTr (None, 28, 28, 1)         577       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 10,358,273\n",
      "Trainable params: 10,357,889\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build and compile the discriminator\n",
    "discriminator = buildDiscriminator()\n",
    "discriminator.compile(loss='binary_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# Build the generator\n",
    "generator = buildGenerator()\n",
    "\n",
    "# The generator takes noise as input and generates imgs\n",
    "z = Input(shape=(100,))\n",
    "img = generator(z)\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# The discriminator takes generated images as input and determines validity\n",
    "valid = discriminator(img)\n",
    "\n",
    "# The combined model  (stacked generator and discriminator)\n",
    "# Trains the generator to fool the discriminator\n",
    "combined = Model(z, valid)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_imgs(epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "    # Load the dataset\n",
    "    (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "    # Rescale -1 to 1\n",
    "    X_train = X_train / 127.5 - 1.\n",
    "    X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "    # Adversarial ground truths\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        # Select a random half of images\n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        imgs = X_train[idx]\n",
    "\n",
    "        # Sample noise and generate a batch of new images\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        gen_imgs = generator.predict(noise)   \n",
    "    \n",
    "        # Train the discriminator (real classified as ones and generated as zeros)\n",
    "        d_loss_real = discriminator.train_on_batch(imgs, valid)\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "\n",
    "        # Train the generator (wants discriminator to mistake images as real)\n",
    "        g_loss = combined.train_on_batch(noise, valid)\n",
    "\n",
    "        # Plot the progress\n",
    "        print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "        # If at save interval => save generated image samples\n",
    "        if epoch % save_interval == 0:\n",
    "            save_imgs(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniyalusmani/anaconda3/lib/python3.6/site-packages/keras/engine/training.py:975: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.748047, acc.: 56.25%] [G loss: 1.481040]\n",
      "1 [D loss: 0.712858, acc.: 60.94%] [G loss: 1.591220]\n",
      "2 [D loss: 0.724625, acc.: 56.25%] [G loss: 1.001560]\n",
      "3 [D loss: 0.653903, acc.: 67.19%] [G loss: 1.156729]\n",
      "4 [D loss: 0.613763, acc.: 65.62%] [G loss: 1.135445]\n",
      "5 [D loss: 0.611107, acc.: 65.62%] [G loss: 0.893878]\n",
      "6 [D loss: 0.520236, acc.: 71.88%] [G loss: 1.159514]\n",
      "7 [D loss: 0.903418, acc.: 51.56%] [G loss: 1.339163]\n",
      "8 [D loss: 1.226409, acc.: 31.25%] [G loss: 1.849523]\n",
      "9 [D loss: 1.174496, acc.: 21.88%] [G loss: 1.624694]\n",
      "10 [D loss: 1.253010, acc.: 25.00%] [G loss: 1.283856]\n",
      "11 [D loss: 0.879671, acc.: 53.12%] [G loss: 1.324402]\n",
      "12 [D loss: 1.204717, acc.: 20.31%] [G loss: 0.951802]\n",
      "13 [D loss: 0.914853, acc.: 48.44%] [G loss: 1.496480]\n",
      "14 [D loss: 0.927349, acc.: 43.75%] [G loss: 1.372675]\n",
      "15 [D loss: 1.078593, acc.: 31.25%] [G loss: 1.181622]\n",
      "16 [D loss: 0.919915, acc.: 50.00%] [G loss: 0.803636]\n",
      "17 [D loss: 0.647046, acc.: 62.50%] [G loss: 0.851588]\n",
      "18 [D loss: 0.723943, acc.: 59.38%] [G loss: 1.045247]\n",
      "19 [D loss: 0.882521, acc.: 51.56%] [G loss: 1.195984]\n",
      "20 [D loss: 0.895101, acc.: 40.62%] [G loss: 1.319921]\n",
      "21 [D loss: 1.224726, acc.: 29.69%] [G loss: 1.605626]\n",
      "22 [D loss: 1.444392, acc.: 29.69%] [G loss: 1.185547]\n",
      "23 [D loss: 1.213484, acc.: 34.38%] [G loss: 1.210407]\n",
      "24 [D loss: 0.905706, acc.: 45.31%] [G loss: 1.005878]\n",
      "25 [D loss: 0.888978, acc.: 51.56%] [G loss: 1.076134]\n",
      "26 [D loss: 0.931158, acc.: 51.56%] [G loss: 1.196942]\n",
      "27 [D loss: 0.954661, acc.: 45.31%] [G loss: 1.173828]\n",
      "28 [D loss: 0.973108, acc.: 39.06%] [G loss: 1.465107]\n",
      "29 [D loss: 0.980451, acc.: 37.50%] [G loss: 1.449190]\n",
      "30 [D loss: 0.801396, acc.: 56.25%] [G loss: 1.174060]\n",
      "31 [D loss: 0.807082, acc.: 48.44%] [G loss: 0.993352]\n",
      "32 [D loss: 0.714522, acc.: 56.25%] [G loss: 1.085716]\n",
      "33 [D loss: 0.744072, acc.: 53.12%] [G loss: 1.342915]\n",
      "34 [D loss: 0.781469, acc.: 62.50%] [G loss: 1.350772]\n",
      "35 [D loss: 1.054639, acc.: 35.94%] [G loss: 1.262286]\n",
      "36 [D loss: 0.900733, acc.: 37.50%] [G loss: 1.482127]\n",
      "37 [D loss: 1.028531, acc.: 40.62%] [G loss: 1.302570]\n",
      "38 [D loss: 0.917957, acc.: 34.38%] [G loss: 1.351130]\n",
      "39 [D loss: 0.840050, acc.: 46.88%] [G loss: 1.188045]\n",
      "40 [D loss: 0.836121, acc.: 53.12%] [G loss: 1.322574]\n",
      "41 [D loss: 0.774901, acc.: 53.12%] [G loss: 1.377732]\n",
      "42 [D loss: 1.073284, acc.: 34.38%] [G loss: 1.334506]\n",
      "43 [D loss: 0.894341, acc.: 50.00%] [G loss: 1.471743]\n",
      "44 [D loss: 0.972063, acc.: 46.88%] [G loss: 1.308571]\n",
      "45 [D loss: 0.653242, acc.: 70.31%] [G loss: 1.147058]\n",
      "46 [D loss: 0.752951, acc.: 59.38%] [G loss: 1.031660]\n",
      "47 [D loss: 0.846760, acc.: 45.31%] [G loss: 1.029404]\n",
      "48 [D loss: 0.896778, acc.: 40.62%] [G loss: 1.281476]\n",
      "49 [D loss: 0.959450, acc.: 43.75%] [G loss: 1.170737]\n",
      "50 [D loss: 1.059886, acc.: 34.38%] [G loss: 1.209010]\n",
      "51 [D loss: 0.912972, acc.: 46.88%] [G loss: 1.250587]\n",
      "52 [D loss: 0.926518, acc.: 40.62%] [G loss: 1.451903]\n",
      "53 [D loss: 0.859192, acc.: 45.31%] [G loss: 1.058572]\n",
      "54 [D loss: 0.773100, acc.: 57.81%] [G loss: 0.929637]\n",
      "55 [D loss: 0.837903, acc.: 59.38%] [G loss: 1.165149]\n",
      "56 [D loss: 0.661112, acc.: 64.06%] [G loss: 1.650213]\n",
      "57 [D loss: 0.761869, acc.: 50.00%] [G loss: 1.158883]\n",
      "58 [D loss: 1.183088, acc.: 34.38%] [G loss: 1.078934]\n",
      "59 [D loss: 0.974264, acc.: 37.50%] [G loss: 1.162630]\n",
      "60 [D loss: 0.810652, acc.: 46.88%] [G loss: 1.087406]\n",
      "61 [D loss: 0.870535, acc.: 48.44%] [G loss: 0.950439]\n",
      "62 [D loss: 0.763769, acc.: 51.56%] [G loss: 1.249762]\n",
      "63 [D loss: 0.895127, acc.: 45.31%] [G loss: 1.126792]\n",
      "64 [D loss: 0.895215, acc.: 50.00%] [G loss: 1.037294]\n",
      "65 [D loss: 1.042169, acc.: 37.50%] [G loss: 1.278895]\n",
      "66 [D loss: 1.004644, acc.: 40.62%] [G loss: 1.151962]\n",
      "67 [D loss: 0.957103, acc.: 37.50%] [G loss: 1.379936]\n",
      "68 [D loss: 0.986278, acc.: 34.38%] [G loss: 1.046853]\n",
      "69 [D loss: 0.825125, acc.: 46.88%] [G loss: 1.079640]\n",
      "70 [D loss: 0.954365, acc.: 45.31%] [G loss: 1.039802]\n",
      "71 [D loss: 0.853534, acc.: 50.00%] [G loss: 1.051214]\n",
      "72 [D loss: 0.897618, acc.: 48.44%] [G loss: 1.005795]\n",
      "73 [D loss: 1.032861, acc.: 35.94%] [G loss: 1.350692]\n",
      "74 [D loss: 0.820456, acc.: 48.44%] [G loss: 1.090932]\n",
      "75 [D loss: 0.862735, acc.: 51.56%] [G loss: 0.938719]\n",
      "76 [D loss: 0.675847, acc.: 65.62%] [G loss: 1.067925]\n",
      "77 [D loss: 0.921862, acc.: 45.31%] [G loss: 1.135561]\n",
      "78 [D loss: 0.870212, acc.: 40.62%] [G loss: 1.086817]\n",
      "79 [D loss: 0.835544, acc.: 50.00%] [G loss: 1.492849]\n",
      "80 [D loss: 0.868479, acc.: 43.75%] [G loss: 1.345085]\n",
      "81 [D loss: 1.005696, acc.: 34.38%] [G loss: 1.099753]\n",
      "82 [D loss: 1.004230, acc.: 40.62%] [G loss: 1.053641]\n",
      "83 [D loss: 0.929748, acc.: 39.06%] [G loss: 0.881295]\n",
      "84 [D loss: 0.873108, acc.: 42.19%] [G loss: 1.128742]\n",
      "85 [D loss: 0.749555, acc.: 57.81%] [G loss: 1.210088]\n",
      "86 [D loss: 0.937866, acc.: 32.81%] [G loss: 1.028039]\n",
      "87 [D loss: 0.940941, acc.: 46.88%] [G loss: 1.065043]\n",
      "88 [D loss: 0.951421, acc.: 35.94%] [G loss: 1.122009]\n",
      "89 [D loss: 0.971277, acc.: 34.38%] [G loss: 1.068681]\n",
      "90 [D loss: 1.055294, acc.: 35.94%] [G loss: 1.082341]\n",
      "91 [D loss: 0.827328, acc.: 50.00%] [G loss: 1.099257]\n",
      "92 [D loss: 1.013689, acc.: 31.25%] [G loss: 1.259600]\n",
      "93 [D loss: 0.801824, acc.: 54.69%] [G loss: 1.228712]\n",
      "94 [D loss: 0.948369, acc.: 42.19%] [G loss: 1.141003]\n",
      "95 [D loss: 0.903345, acc.: 45.31%] [G loss: 1.227457]\n",
      "96 [D loss: 0.782005, acc.: 54.69%] [G loss: 1.487376]\n",
      "97 [D loss: 0.934373, acc.: 35.94%] [G loss: 1.419309]\n",
      "98 [D loss: 0.920054, acc.: 43.75%] [G loss: 1.142195]\n",
      "99 [D loss: 0.987089, acc.: 42.19%] [G loss: 1.299106]\n",
      "100 [D loss: 1.011011, acc.: 35.94%] [G loss: 0.956957]\n",
      "101 [D loss: 0.833753, acc.: 50.00%] [G loss: 1.022871]\n",
      "102 [D loss: 0.931757, acc.: 40.62%] [G loss: 1.054045]\n",
      "103 [D loss: 0.892177, acc.: 45.31%] [G loss: 1.153122]\n",
      "104 [D loss: 0.977967, acc.: 39.06%] [G loss: 1.277881]\n",
      "105 [D loss: 0.752671, acc.: 51.56%] [G loss: 1.030831]\n",
      "106 [D loss: 0.875699, acc.: 45.31%] [G loss: 1.175226]\n",
      "107 [D loss: 0.882056, acc.: 48.44%] [G loss: 1.376281]\n",
      "108 [D loss: 0.798546, acc.: 51.56%] [G loss: 1.027699]\n",
      "109 [D loss: 0.925529, acc.: 42.19%] [G loss: 0.856710]\n",
      "110 [D loss: 0.854755, acc.: 43.75%] [G loss: 0.827043]\n",
      "111 [D loss: 0.838768, acc.: 48.44%] [G loss: 1.072544]\n",
      "112 [D loss: 0.909917, acc.: 40.62%] [G loss: 1.263019]\n",
      "113 [D loss: 0.866363, acc.: 48.44%] [G loss: 1.318188]\n",
      "114 [D loss: 0.886347, acc.: 37.50%] [G loss: 1.313176]\n",
      "115 [D loss: 0.936640, acc.: 35.94%] [G loss: 1.146625]\n",
      "116 [D loss: 0.823735, acc.: 46.88%] [G loss: 1.077736]\n",
      "117 [D loss: 0.935587, acc.: 42.19%] [G loss: 1.183804]\n",
      "118 [D loss: 0.858667, acc.: 40.62%] [G loss: 0.972487]\n",
      "119 [D loss: 0.906191, acc.: 45.31%] [G loss: 1.055441]\n",
      "120 [D loss: 0.836753, acc.: 45.31%] [G loss: 1.051248]\n",
      "121 [D loss: 0.870663, acc.: 45.31%] [G loss: 1.046699]\n",
      "122 [D loss: 0.951273, acc.: 43.75%] [G loss: 1.374673]\n",
      "123 [D loss: 0.863383, acc.: 48.44%] [G loss: 1.375337]\n",
      "124 [D loss: 0.856783, acc.: 53.12%] [G loss: 1.151426]\n",
      "125 [D loss: 0.920666, acc.: 46.88%] [G loss: 0.961172]\n",
      "126 [D loss: 0.831132, acc.: 48.44%] [G loss: 1.190878]\n",
      "127 [D loss: 0.900580, acc.: 46.88%] [G loss: 1.161613]\n",
      "128 [D loss: 0.862511, acc.: 53.12%] [G loss: 1.140489]\n",
      "129 [D loss: 0.880433, acc.: 45.31%] [G loss: 1.195623]\n",
      "130 [D loss: 0.918261, acc.: 50.00%] [G loss: 0.907803]\n",
      "131 [D loss: 0.826443, acc.: 51.56%] [G loss: 1.099746]\n",
      "132 [D loss: 0.856836, acc.: 50.00%] [G loss: 1.022483]\n",
      "133 [D loss: 0.833919, acc.: 54.69%] [G loss: 0.985126]\n",
      "134 [D loss: 0.979725, acc.: 28.12%] [G loss: 1.100035]\n",
      "135 [D loss: 0.910569, acc.: 42.19%] [G loss: 1.191871]\n",
      "136 [D loss: 0.779867, acc.: 42.19%] [G loss: 1.002683]\n",
      "137 [D loss: 0.961713, acc.: 34.38%] [G loss: 0.983952]\n",
      "138 [D loss: 0.927235, acc.: 40.62%] [G loss: 1.004069]\n",
      "139 [D loss: 0.924580, acc.: 42.19%] [G loss: 1.147229]\n",
      "140 [D loss: 0.895581, acc.: 45.31%] [G loss: 1.052975]\n",
      "141 [D loss: 0.860926, acc.: 40.62%] [G loss: 0.915336]\n",
      "142 [D loss: 0.847089, acc.: 48.44%] [G loss: 1.080148]\n",
      "143 [D loss: 0.930148, acc.: 43.75%] [G loss: 0.946626]\n",
      "144 [D loss: 0.892970, acc.: 43.75%] [G loss: 1.187762]\n",
      "145 [D loss: 0.888918, acc.: 43.75%] [G loss: 1.032093]\n",
      "146 [D loss: 0.838792, acc.: 50.00%] [G loss: 1.316303]\n",
      "147 [D loss: 0.726472, acc.: 51.56%] [G loss: 1.177919]\n",
      "148 [D loss: 1.028289, acc.: 25.00%] [G loss: 1.151051]\n",
      "149 [D loss: 0.877977, acc.: 46.88%] [G loss: 0.934973]\n",
      "150 [D loss: 0.892624, acc.: 45.31%] [G loss: 0.963853]\n",
      "151 [D loss: 0.824628, acc.: 48.44%] [G loss: 1.063629]\n",
      "152 [D loss: 0.985147, acc.: 35.94%] [G loss: 0.998689]\n",
      "153 [D loss: 0.881564, acc.: 37.50%] [G loss: 1.136773]\n",
      "154 [D loss: 0.923883, acc.: 40.62%] [G loss: 1.139831]\n",
      "155 [D loss: 0.902337, acc.: 39.06%] [G loss: 0.971745]\n",
      "156 [D loss: 0.924624, acc.: 32.81%] [G loss: 1.005917]\n",
      "157 [D loss: 0.787334, acc.: 53.12%] [G loss: 0.961567]\n",
      "158 [D loss: 0.976397, acc.: 34.38%] [G loss: 1.044265]\n",
      "159 [D loss: 0.989798, acc.: 39.06%] [G loss: 0.946555]\n",
      "160 [D loss: 0.764092, acc.: 57.81%] [G loss: 1.028318]\n",
      "161 [D loss: 0.870553, acc.: 43.75%] [G loss: 0.951709]\n",
      "162 [D loss: 0.986354, acc.: 43.75%] [G loss: 1.043073]\n",
      "163 [D loss: 0.818766, acc.: 46.88%] [G loss: 1.299095]\n",
      "164 [D loss: 0.892988, acc.: 50.00%] [G loss: 1.074343]\n",
      "165 [D loss: 0.919039, acc.: 40.62%] [G loss: 0.952649]\n",
      "166 [D loss: 0.794818, acc.: 43.75%] [G loss: 1.289376]\n",
      "167 [D loss: 0.950587, acc.: 42.19%] [G loss: 1.069565]\n",
      "168 [D loss: 0.833583, acc.: 45.31%] [G loss: 1.037236]\n",
      "169 [D loss: 0.865312, acc.: 43.75%] [G loss: 0.930586]\n",
      "170 [D loss: 0.944351, acc.: 34.38%] [G loss: 0.897721]\n",
      "171 [D loss: 0.833636, acc.: 45.31%] [G loss: 0.981201]\n",
      "172 [D loss: 0.789532, acc.: 48.44%] [G loss: 1.120458]\n",
      "173 [D loss: 0.850381, acc.: 46.88%] [G loss: 1.085849]\n",
      "174 [D loss: 0.866593, acc.: 40.62%] [G loss: 0.941310]\n",
      "175 [D loss: 0.675314, acc.: 51.56%] [G loss: 1.172596]\n",
      "176 [D loss: 0.775565, acc.: 50.00%] [G loss: 1.156315]\n",
      "177 [D loss: 0.803461, acc.: 50.00%] [G loss: 1.041183]\n",
      "178 [D loss: 0.854644, acc.: 45.31%] [G loss: 1.064358]\n",
      "179 [D loss: 0.924050, acc.: 37.50%] [G loss: 1.222170]\n",
      "180 [D loss: 0.835529, acc.: 53.12%] [G loss: 1.062580]\n",
      "181 [D loss: 0.816794, acc.: 48.44%] [G loss: 0.982054]\n",
      "182 [D loss: 0.859149, acc.: 46.88%] [G loss: 1.052382]\n",
      "183 [D loss: 0.872259, acc.: 43.75%] [G loss: 1.054061]\n",
      "184 [D loss: 0.781467, acc.: 54.69%] [G loss: 1.029462]\n",
      "185 [D loss: 0.900207, acc.: 35.94%] [G loss: 1.103085]\n",
      "186 [D loss: 0.902238, acc.: 40.62%] [G loss: 0.925176]\n",
      "187 [D loss: 0.836230, acc.: 51.56%] [G loss: 1.103555]\n",
      "188 [D loss: 0.743599, acc.: 56.25%] [G loss: 1.013572]\n",
      "189 [D loss: 0.905079, acc.: 43.75%] [G loss: 1.034764]\n",
      "190 [D loss: 0.950838, acc.: 39.06%] [G loss: 0.876227]\n",
      "191 [D loss: 0.950454, acc.: 35.94%] [G loss: 0.933949]\n",
      "192 [D loss: 0.861006, acc.: 39.06%] [G loss: 1.020260]\n",
      "193 [D loss: 0.842394, acc.: 48.44%] [G loss: 1.079785]\n",
      "194 [D loss: 0.869308, acc.: 43.75%] [G loss: 1.015150]\n",
      "195 [D loss: 0.975784, acc.: 37.50%] [G loss: 1.144241]\n",
      "196 [D loss: 0.887399, acc.: 46.88%] [G loss: 1.024787]\n",
      "197 [D loss: 0.864254, acc.: 42.19%] [G loss: 1.058125]\n",
      "198 [D loss: 0.964722, acc.: 31.25%] [G loss: 0.980311]\n",
      "199 [D loss: 0.726320, acc.: 53.12%] [G loss: 0.881012]\n"
     ]
    }
   ],
   "source": [
    "train(epochs=200, batch_size=32, save_interval=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
