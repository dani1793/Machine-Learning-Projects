{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adveserial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative Adveserial Networks (GAN) are type of neural network architecture that is originally designed to generate samples which are close to orignal sample space. As the name suggests it is Generative network, which means that there is no supervised learning required. The general structure of GANs is provided in figure below. The figure shows two different networks present in the framework. They are commonly named Generator Network and Discriminator Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two networks could be categorized as adversaries of each other. The basic purpose of the Generator Network is to learn the data distribution which is almost equivalent to original data distribution. The Generator Network starts with the noisy distribution and starts to learn from it. The network learns from the feedback provided by the Discriminator Network. The Discriminator could generally be any function that is differentiable. The network is trained on the sample latent data that have dimensions less than or equal to that of original data. Using this strategy allows the Generator to learn so generic features and represent the sample space in less dimesions. This ease the way to create further samples which are not in the original space. Inputs to the generator are randomly sampled from the modelâ€™s prior over the latent variables.\n",
    "\n",
    "The Discriminator Network is provided with two types of data; the original data and the noisy data (i.e: data from Generator Network). Discriminator Network could be thought of as a binary classifier. The task of this binary classifier is to distinguish original data from generated data. It outputs 1 when the classifier thinks that the data is from original distribution and it outputs 0 if the classifier thinks that the data is from Generator Network.\n",
    "\n",
    "Overall framework of GAN could be seen as a two player game where Generator Network is trying to get past the Discriminator Network by making it believe that the generated sample is from the original distibution. On the other hand Discriminator Network tries it best to not be fooled by Generative Network. In Ideal case If both models have sufficient capacity, then the Nash equilibrium of this game corresponds to the G(z) being drawn from the same distribution as the training data, and D(x) = 1/2 for all x.\n",
    "\n",
    "**The GANs have mostly similar to VAEs, the most salient difference is that, if relying on standard backprop, VAEs cannot have discrete variables at the input to the generator, while GANs cannot have discrete variables at the output of the generator. The GAN approximation is subject to the failures of supervised learning: overfitting and underfitting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Cost Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There could be two types of training, for the first part of the training the Discriminator is fed with the original data. The data has half fake and half original data samples. The goal of Descriminator is to identify as much original data samples and output 1. \n",
    "\n",
    "In second part, generator is fed with latent variables from model's prior. The Generator tries to create fake data items and those data items are passed to Discriminator Network. In this part, the goal of Generator is to keep the output near to 1 whereas Discriminator tries to keep output near 0.\n",
    "\n",
    "There are two cost functions for this framework. One for Generative Network and one for Discriminator Network. The commonly used cost function for Discriminator Network is modified cross entrophy, where it is expected that there are two types on inputs.\n",
    "\n",
    "$$C_{D} = -\\frac{1}{2}\\mathbb{E}_{x\\sim p(data)}log(D(x)) -\\frac{1}{2}\\mathbb{E}_{z}log(1- D(G(z)))$$\n",
    "\n",
    "As could be seen from equation above, it maximize the data samples with are from original data and minimize the samples which are generated from Generator Network.\n",
    "\n",
    "By training the discriminator we get the estimated ration of the two models\n",
    "\n",
    "$$\\frac{p_{data}(x)}{p_{model}(x)}$$\n",
    "\n",
    "This is key approximation technqiue that sets GANs apart form variational autoencoders and Boltzmann machines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for Generator Network a simple cost function would be negative of the Discriminator cost function \n",
    "\n",
    "$$C_{G} = - C_{D}$$\n",
    "\n",
    "The advantage of this approch is that we would only need to calculate the value function for adversary game. We can than find minmax function for one cost function and that would optimize the whole framework.\n",
    "\n",
    "$$\\theta_{G} = \\text{argmin}_{\\theta^{(G)}}  \\text{max}_{\\theta^{(D)}}C_{G}$$\n",
    "\n",
    "However, there is also a big disadvantage of this cost function. Imagine a scenerio where discriminator successfully rejects generator samples with high confidence, in this case the generators's gradients vanishes. This could be intuitively seen in the above equation of cost function for discriminator. When D(G(z)) is 0 or near zero than the cost is almost zero, and taking derivative with zero initial value will vanish the gradients before reaching the input layer of neural network.\n",
    "\n",
    "To overcome this problem we use a different cost function for Generator network.\n",
    "\n",
    "$$C_{G} = -\\frac{1}{2}\\mathbb{E}_{z}log(D(G(z)))$$\n",
    "\n",
    "Intuiviely this means that the generator maximizes the logprobability\n",
    "of the discriminator being mistaken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep, Convolutional GAN (DCGAN), is the common architecture used in modern applications. DCGANs were initially used to generate high resolution images. They perform this task in one iteration where as prior to DCGANs, LAPGANs were used which required generation of image multiples in one iteration.\n",
    "\n",
    "The best practicies for training DCGAN can be found on the following side https://github.com/soumith/ganhacks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get packges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniyalusmani/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D, Conv2DTranspose\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define initial variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input shape\n",
    "img_rows = 28\n",
    "img_cols = 28\n",
    "channels = 1\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "latent_dim = 100\n",
    "\n",
    "optimizer = Adam(0.0002, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper of Radford et al. gives some hints about what is a good DCGAN architecture :\n",
    "\n",
    "* Replace any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator). In other words, when the generator needs to increase the spatial dimension of the representation, it uses transposed convolution with a stride greater than 1\n",
    "\n",
    "* Use Batch Normalization in both the generator (except at the output layer) and the discriminator (except at the input layer)\n",
    "* Remove fully connected hidden layers for deeper architectures\n",
    "* Use ReLU activation in generator for all layers except for the output\n",
    "* Use LeakyReLU activation in the discriminator for all layers\n",
    "\n",
    "For creation of generator, we have used Transposed conv2D layer with batch normalization applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./files/dc-gan-flow.png\" \n",
    "alt=\"IMAGE ALT TEXT HERE\" border=\"10\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildGenerator():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(128 * 28 * 28, activation=\"relu\", input_dim=latent_dim))\n",
    "    model.add(Reshape((28, 28, 128)))\n",
    "    model.add(Conv2DTranspose(128, kernel_size=3, padding=\"same\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Conv2DTranspose(64, kernel_size=3, padding=\"same\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Conv2DTranspose(channels, kernel_size=3, padding=\"same\"))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    noise = Input(shape=(latent_dim,))\n",
    "    img = model(noise)\n",
    "\n",
    "    return Model(noise, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discriminator is\n",
    "generated using the Conv2D layer, with activation function set to LeakyReLU and batch Normalization is applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildDiscriminator():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=img_shape, padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    img = Input(shape=img_shape)\n",
    "    validity = model(img)\n",
    "\n",
    "    return Model(img, validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 4097      \n",
      "=================================================================\n",
      "Total params: 393,729\n",
      "Trainable params: 392,833\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 100352)            10135552  \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 28, 28, 128)       147584    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 28, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 64)        73792     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 28, 28, 1)         577       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 10,358,273\n",
      "Trainable params: 10,357,889\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build and compile the discriminator\n",
    "discriminator = buildDiscriminator()\n",
    "discriminator.compile(loss='binary_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# Build the generator\n",
    "generator = buildGenerator()\n",
    "\n",
    "# The generator takes noise as input and generates imgs\n",
    "z = Input(shape=(100,))\n",
    "img = generator(z)\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# The discriminator takes generated images as input and determines validity\n",
    "valid = discriminator(img)\n",
    "\n",
    "# The combined model  (stacked generator and discriminator)\n",
    "# Trains the generator to fool the discriminator\n",
    "combined = Model(z, valid)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_imgs(epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "    # Load the dataset\n",
    "    (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "    # Rescale -1 to 1\n",
    "    X_train = X_train / 127.5 - 1.\n",
    "    X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "    # Adversarial ground truths\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        # Select a random half of images\n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        imgs = X_train[idx]\n",
    "\n",
    "        # Sample noise and generate a batch of new images\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        gen_imgs = generator.predict(noise)   \n",
    "    \n",
    "        # Train the discriminator (real classified as ones and generated as zeros)\n",
    "        d_loss_real = discriminator.train_on_batch(imgs, valid)\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "\n",
    "        # Train the generator (wants discriminator to mistake images as real)\n",
    "        g_loss = combined.train_on_batch(noise, valid)\n",
    "\n",
    "        # Plot the progress\n",
    "        print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "        # If at save interval => save generated image samples\n",
    "        if epoch % save_interval == 0:\n",
    "            save_imgs(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniyalusmani/anaconda3/lib/python3.6/site-packages/keras/engine/training.py:975: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.750284, acc.: 50.00%] [G loss: 0.700566]\n",
      "1 [D loss: 0.422570, acc.: 82.81%] [G loss: 1.015291]\n",
      "2 [D loss: 0.509268, acc.: 70.31%] [G loss: 0.697980]\n",
      "3 [D loss: 0.512294, acc.: 75.00%] [G loss: 0.756566]\n",
      "4 [D loss: 0.702772, acc.: 60.94%] [G loss: 0.894716]\n",
      "5 [D loss: 0.616657, acc.: 67.19%] [G loss: 0.880327]\n",
      "6 [D loss: 0.584337, acc.: 76.56%] [G loss: 0.709457]\n",
      "7 [D loss: 0.940414, acc.: 46.88%] [G loss: 0.564096]\n",
      "8 [D loss: 0.754347, acc.: 60.94%] [G loss: 1.446072]\n",
      "9 [D loss: 0.946122, acc.: 40.62%] [G loss: 0.418355]\n",
      "10 [D loss: 0.626583, acc.: 64.06%] [G loss: 0.948578]\n",
      "11 [D loss: 0.582154, acc.: 71.88%] [G loss: 1.345667]\n",
      "12 [D loss: 0.708845, acc.: 57.81%] [G loss: 0.627676]\n",
      "13 [D loss: 0.708231, acc.: 57.81%] [G loss: 0.771116]\n",
      "14 [D loss: 0.465311, acc.: 81.25%] [G loss: 1.057621]\n",
      "15 [D loss: 0.520009, acc.: 71.88%] [G loss: 1.522194]\n",
      "16 [D loss: 0.820395, acc.: 51.56%] [G loss: 1.256114]\n",
      "17 [D loss: 0.788396, acc.: 50.00%] [G loss: 1.679573]\n",
      "18 [D loss: 1.211594, acc.: 28.12%] [G loss: 1.038150]\n",
      "19 [D loss: 0.785299, acc.: 56.25%] [G loss: 1.110119]\n",
      "20 [D loss: 0.645735, acc.: 65.62%] [G loss: 1.097800]\n",
      "21 [D loss: 0.902325, acc.: 42.19%] [G loss: 1.191849]\n",
      "22 [D loss: 0.722731, acc.: 56.25%] [G loss: 1.732307]\n",
      "23 [D loss: 0.654910, acc.: 68.75%] [G loss: 1.118519]\n",
      "24 [D loss: 0.811799, acc.: 48.44%] [G loss: 1.174576]\n",
      "25 [D loss: 0.694012, acc.: 62.50%] [G loss: 1.107790]\n",
      "26 [D loss: 0.551214, acc.: 75.00%] [G loss: 1.152370]\n",
      "27 [D loss: 0.542144, acc.: 71.88%] [G loss: 1.197170]\n",
      "28 [D loss: 0.651744, acc.: 71.88%] [G loss: 1.071214]\n",
      "29 [D loss: 0.572587, acc.: 71.88%] [G loss: 0.966957]\n",
      "30 [D loss: 0.915949, acc.: 43.75%] [G loss: 0.577285]\n",
      "31 [D loss: 0.632040, acc.: 64.06%] [G loss: 1.047595]\n",
      "32 [D loss: 0.516115, acc.: 78.12%] [G loss: 1.179900]\n",
      "33 [D loss: 0.616175, acc.: 64.06%] [G loss: 0.805771]\n",
      "34 [D loss: 0.590879, acc.: 67.19%] [G loss: 0.824775]\n",
      "35 [D loss: 0.662072, acc.: 62.50%] [G loss: 0.668222]\n",
      "36 [D loss: 0.760444, acc.: 59.38%] [G loss: 0.744396]\n",
      "37 [D loss: 0.751225, acc.: 62.50%] [G loss: 1.009580]\n",
      "38 [D loss: 0.632070, acc.: 57.81%] [G loss: 1.201941]\n",
      "39 [D loss: 0.810629, acc.: 59.38%] [G loss: 0.938779]\n",
      "40 [D loss: 0.718411, acc.: 57.81%] [G loss: 1.154013]\n",
      "41 [D loss: 0.449207, acc.: 81.25%] [G loss: 1.227744]\n",
      "42 [D loss: 0.534503, acc.: 70.31%] [G loss: 1.098399]\n",
      "43 [D loss: 0.444415, acc.: 84.38%] [G loss: 1.274616]\n",
      "44 [D loss: 0.761710, acc.: 57.81%] [G loss: 1.069934]\n",
      "45 [D loss: 0.517737, acc.: 75.00%] [G loss: 1.303057]\n",
      "46 [D loss: 0.445119, acc.: 79.69%] [G loss: 1.447552]\n",
      "47 [D loss: 0.826329, acc.: 46.88%] [G loss: 0.933435]\n",
      "48 [D loss: 0.838529, acc.: 48.44%] [G loss: 1.549401]\n",
      "49 [D loss: 0.519139, acc.: 76.56%] [G loss: 1.740212]\n",
      "50 [D loss: 0.635896, acc.: 60.94%] [G loss: 1.201300]\n",
      "51 [D loss: 0.508899, acc.: 78.12%] [G loss: 1.353582]\n",
      "52 [D loss: 0.341250, acc.: 89.06%] [G loss: 1.063366]\n",
      "53 [D loss: 0.492245, acc.: 78.12%] [G loss: 0.836301]\n",
      "54 [D loss: 0.619243, acc.: 68.75%] [G loss: 0.668487]\n",
      "55 [D loss: 0.411148, acc.: 85.94%] [G loss: 0.685593]\n",
      "56 [D loss: 0.551615, acc.: 68.75%] [G loss: 0.856864]\n",
      "57 [D loss: 0.721646, acc.: 62.50%] [G loss: 0.818507]\n",
      "58 [D loss: 0.469036, acc.: 79.69%] [G loss: 1.035524]\n",
      "59 [D loss: 0.814180, acc.: 48.44%] [G loss: 1.008531]\n",
      "60 [D loss: 0.652825, acc.: 59.38%] [G loss: 0.873130]\n",
      "61 [D loss: 0.826780, acc.: 48.44%] [G loss: 0.685192]\n",
      "62 [D loss: 0.562238, acc.: 75.00%] [G loss: 1.349403]\n",
      "63 [D loss: 0.632441, acc.: 67.19%] [G loss: 1.058020]\n",
      "64 [D loss: 0.595496, acc.: 65.62%] [G loss: 1.193852]\n",
      "65 [D loss: 0.952348, acc.: 43.75%] [G loss: 1.187425]\n",
      "66 [D loss: 0.564271, acc.: 68.75%] [G loss: 1.294539]\n",
      "67 [D loss: 0.575245, acc.: 75.00%] [G loss: 1.483850]\n",
      "68 [D loss: 0.669517, acc.: 65.62%] [G loss: 1.381580]\n",
      "69 [D loss: 0.799741, acc.: 53.12%] [G loss: 0.775970]\n",
      "70 [D loss: 0.944039, acc.: 43.75%] [G loss: 1.106928]\n",
      "71 [D loss: 0.777156, acc.: 48.44%] [G loss: 1.196735]\n",
      "72 [D loss: 0.661019, acc.: 64.06%] [G loss: 1.275270]\n",
      "73 [D loss: 0.676395, acc.: 65.62%] [G loss: 1.178982]\n",
      "74 [D loss: 0.412155, acc.: 85.94%] [G loss: 1.020428]\n",
      "75 [D loss: 0.534861, acc.: 71.88%] [G loss: 0.968490]\n",
      "76 [D loss: 0.673648, acc.: 56.25%] [G loss: 0.994345]\n",
      "77 [D loss: 0.669017, acc.: 54.69%] [G loss: 0.903987]\n",
      "78 [D loss: 0.651765, acc.: 65.62%] [G loss: 1.082948]\n",
      "79 [D loss: 0.733063, acc.: 60.94%] [G loss: 0.660176]\n",
      "80 [D loss: 0.443391, acc.: 79.69%] [G loss: 1.096582]\n",
      "81 [D loss: 0.472699, acc.: 71.88%] [G loss: 0.970489]\n",
      "82 [D loss: 0.588785, acc.: 71.88%] [G loss: 0.781256]\n",
      "83 [D loss: 0.660348, acc.: 64.06%] [G loss: 0.643647]\n",
      "84 [D loss: 0.460479, acc.: 79.69%] [G loss: 0.919160]\n",
      "85 [D loss: 0.603642, acc.: 60.94%] [G loss: 0.793120]\n",
      "86 [D loss: 0.581035, acc.: 68.75%] [G loss: 0.941271]\n",
      "87 [D loss: 0.604883, acc.: 68.75%] [G loss: 1.115539]\n",
      "88 [D loss: 0.532890, acc.: 73.44%] [G loss: 0.749487]\n",
      "89 [D loss: 0.490877, acc.: 79.69%] [G loss: 0.617525]\n",
      "90 [D loss: 0.486990, acc.: 79.69%] [G loss: 0.663087]\n",
      "91 [D loss: 0.614547, acc.: 68.75%] [G loss: 0.645967]\n",
      "92 [D loss: 0.358712, acc.: 85.94%] [G loss: 0.888052]\n",
      "93 [D loss: 0.702868, acc.: 59.38%] [G loss: 0.540300]\n",
      "94 [D loss: 0.706493, acc.: 65.62%] [G loss: 1.016508]\n",
      "95 [D loss: 0.756933, acc.: 59.38%] [G loss: 0.919122]\n",
      "96 [D loss: 0.574729, acc.: 68.75%] [G loss: 1.146015]\n",
      "97 [D loss: 0.692899, acc.: 56.25%] [G loss: 0.919562]\n",
      "98 [D loss: 0.654216, acc.: 64.06%] [G loss: 0.866136]\n",
      "99 [D loss: 0.570726, acc.: 68.75%] [G loss: 1.212267]\n",
      "100 [D loss: 0.633968, acc.: 64.06%] [G loss: 1.023516]\n",
      "101 [D loss: 0.758306, acc.: 51.56%] [G loss: 0.745591]\n",
      "102 [D loss: 0.928094, acc.: 50.00%] [G loss: 1.353528]\n",
      "103 [D loss: 0.645590, acc.: 64.06%] [G loss: 1.505020]\n",
      "104 [D loss: 0.810119, acc.: 53.12%] [G loss: 1.334876]\n",
      "105 [D loss: 0.664754, acc.: 60.94%] [G loss: 1.433377]\n",
      "106 [D loss: 0.764697, acc.: 53.12%] [G loss: 1.066873]\n",
      "107 [D loss: 0.588389, acc.: 71.88%] [G loss: 1.176979]\n",
      "108 [D loss: 0.494829, acc.: 75.00%] [G loss: 1.241155]\n",
      "109 [D loss: 0.558809, acc.: 68.75%] [G loss: 1.233935]\n",
      "110 [D loss: 0.534808, acc.: 75.00%] [G loss: 1.387197]\n",
      "111 [D loss: 0.451715, acc.: 78.12%] [G loss: 1.318922]\n",
      "112 [D loss: 0.559114, acc.: 75.00%] [G loss: 1.127677]\n",
      "113 [D loss: 0.751685, acc.: 57.81%] [G loss: 1.796182]\n",
      "114 [D loss: 0.596597, acc.: 67.19%] [G loss: 1.661710]\n",
      "115 [D loss: 0.750530, acc.: 67.19%] [G loss: 0.977837]\n",
      "116 [D loss: 0.617764, acc.: 71.88%] [G loss: 1.153728]\n",
      "117 [D loss: 0.556949, acc.: 67.19%] [G loss: 1.160132]\n",
      "118 [D loss: 0.464510, acc.: 76.56%] [G loss: 1.070777]\n",
      "119 [D loss: 0.718479, acc.: 56.25%] [G loss: 0.810063]\n",
      "120 [D loss: 0.387620, acc.: 85.94%] [G loss: 1.351690]\n",
      "121 [D loss: 0.777787, acc.: 56.25%] [G loss: 0.936728]\n",
      "122 [D loss: 0.625801, acc.: 67.19%] [G loss: 1.038665]\n",
      "123 [D loss: 0.615086, acc.: 70.31%] [G loss: 1.169574]\n",
      "124 [D loss: 0.700897, acc.: 60.94%] [G loss: 0.911094]\n",
      "125 [D loss: 0.571818, acc.: 71.88%] [G loss: 1.034220]\n",
      "126 [D loss: 0.654869, acc.: 59.38%] [G loss: 1.262702]\n",
      "127 [D loss: 0.735854, acc.: 56.25%] [G loss: 1.028274]\n",
      "128 [D loss: 0.766761, acc.: 56.25%] [G loss: 0.867760]\n",
      "129 [D loss: 0.549424, acc.: 67.19%] [G loss: 1.449391]\n",
      "130 [D loss: 0.692005, acc.: 62.50%] [G loss: 1.435459]\n",
      "131 [D loss: 0.946363, acc.: 50.00%] [G loss: 1.038891]\n",
      "132 [D loss: 0.714629, acc.: 64.06%] [G loss: 1.188216]\n",
      "133 [D loss: 0.681448, acc.: 54.69%] [G loss: 1.195258]\n",
      "134 [D loss: 0.763258, acc.: 51.56%] [G loss: 1.071079]\n",
      "135 [D loss: 0.626428, acc.: 62.50%] [G loss: 1.059312]\n",
      "136 [D loss: 0.779143, acc.: 53.12%] [G loss: 1.015975]\n",
      "137 [D loss: 0.616478, acc.: 62.50%] [G loss: 0.888254]\n",
      "138 [D loss: 0.732560, acc.: 57.81%] [G loss: 0.827724]\n",
      "139 [D loss: 0.566730, acc.: 73.44%] [G loss: 1.583512]\n",
      "140 [D loss: 0.570087, acc.: 65.62%] [G loss: 1.295454]\n",
      "141 [D loss: 0.719327, acc.: 53.12%] [G loss: 1.804515]\n",
      "142 [D loss: 0.667749, acc.: 59.38%] [G loss: 1.680577]\n",
      "143 [D loss: 0.640506, acc.: 57.81%] [G loss: 0.865408]\n",
      "144 [D loss: 0.688085, acc.: 62.50%] [G loss: 1.186080]\n",
      "145 [D loss: 0.428503, acc.: 81.25%] [G loss: 1.438065]\n",
      "146 [D loss: 0.684422, acc.: 62.50%] [G loss: 0.690548]\n",
      "147 [D loss: 0.640766, acc.: 59.38%] [G loss: 0.893422]\n",
      "148 [D loss: 0.553129, acc.: 75.00%] [G loss: 1.548763]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149 [D loss: 0.682671, acc.: 65.62%] [G loss: 1.045471]\n",
      "150 [D loss: 0.694060, acc.: 60.94%] [G loss: 0.960235]\n",
      "151 [D loss: 0.607461, acc.: 67.19%] [G loss: 0.811827]\n",
      "152 [D loss: 0.695574, acc.: 60.94%] [G loss: 0.861982]\n",
      "153 [D loss: 0.723855, acc.: 62.50%] [G loss: 0.790639]\n",
      "154 [D loss: 0.879635, acc.: 54.69%] [G loss: 0.821891]\n",
      "155 [D loss: 0.577335, acc.: 67.19%] [G loss: 1.349121]\n",
      "156 [D loss: 0.633316, acc.: 65.62%] [G loss: 1.048312]\n",
      "157 [D loss: 0.718648, acc.: 62.50%] [G loss: 0.697412]\n",
      "158 [D loss: 0.705160, acc.: 56.25%] [G loss: 1.033154]\n",
      "159 [D loss: 0.863859, acc.: 46.88%] [G loss: 1.154215]\n",
      "160 [D loss: 0.562787, acc.: 71.88%] [G loss: 0.957323]\n",
      "161 [D loss: 0.807699, acc.: 51.56%] [G loss: 1.143473]\n",
      "162 [D loss: 0.706562, acc.: 60.94%] [G loss: 1.216813]\n",
      "163 [D loss: 0.582858, acc.: 64.06%] [G loss: 1.471809]\n",
      "164 [D loss: 0.600518, acc.: 65.62%] [G loss: 0.951359]\n",
      "165 [D loss: 0.711669, acc.: 62.50%] [G loss: 1.264914]\n",
      "166 [D loss: 0.435961, acc.: 81.25%] [G loss: 1.718141]\n",
      "167 [D loss: 0.583340, acc.: 70.31%] [G loss: 1.304873]\n",
      "168 [D loss: 0.642865, acc.: 67.19%] [G loss: 0.970019]\n",
      "169 [D loss: 0.667649, acc.: 59.38%] [G loss: 1.320798]\n",
      "170 [D loss: 0.749820, acc.: 60.94%] [G loss: 1.576173]\n",
      "171 [D loss: 0.572342, acc.: 65.62%] [G loss: 1.091306]\n",
      "172 [D loss: 0.670541, acc.: 51.56%] [G loss: 1.087650]\n",
      "173 [D loss: 0.482299, acc.: 78.12%] [G loss: 1.182424]\n",
      "174 [D loss: 0.731813, acc.: 56.25%] [G loss: 1.204335]\n",
      "175 [D loss: 0.702259, acc.: 57.81%] [G loss: 1.109386]\n",
      "176 [D loss: 0.628918, acc.: 64.06%] [G loss: 1.133765]\n",
      "177 [D loss: 0.672727, acc.: 62.50%] [G loss: 0.742637]\n",
      "178 [D loss: 0.602221, acc.: 64.06%] [G loss: 0.840470]\n",
      "179 [D loss: 0.581836, acc.: 67.19%] [G loss: 0.935657]\n",
      "180 [D loss: 0.477947, acc.: 78.12%] [G loss: 1.114538]\n",
      "181 [D loss: 0.483116, acc.: 75.00%] [G loss: 1.334125]\n",
      "182 [D loss: 0.781191, acc.: 50.00%] [G loss: 1.573649]\n",
      "183 [D loss: 0.624127, acc.: 59.38%] [G loss: 1.413119]\n",
      "184 [D loss: 0.619428, acc.: 57.81%] [G loss: 1.321657]\n",
      "185 [D loss: 0.636212, acc.: 68.75%] [G loss: 1.333564]\n",
      "186 [D loss: 0.629805, acc.: 59.38%] [G loss: 1.173590]\n",
      "187 [D loss: 0.545402, acc.: 68.75%] [G loss: 1.155502]\n",
      "188 [D loss: 0.609547, acc.: 62.50%] [G loss: 0.838426]\n",
      "189 [D loss: 0.696859, acc.: 57.81%] [G loss: 0.993736]\n",
      "190 [D loss: 0.698180, acc.: 67.19%] [G loss: 1.059024]\n",
      "191 [D loss: 0.508276, acc.: 75.00%] [G loss: 1.021105]\n",
      "192 [D loss: 0.617769, acc.: 67.19%] [G loss: 1.072622]\n",
      "193 [D loss: 0.607927, acc.: 65.62%] [G loss: 0.967639]\n",
      "194 [D loss: 0.459043, acc.: 76.56%] [G loss: 1.125971]\n",
      "195 [D loss: 0.653976, acc.: 60.94%] [G loss: 0.828178]\n",
      "196 [D loss: 0.762842, acc.: 56.25%] [G loss: 0.789000]\n",
      "197 [D loss: 0.664181, acc.: 62.50%] [G loss: 1.048721]\n",
      "198 [D loss: 1.033910, acc.: 42.19%] [G loss: 1.427847]\n",
      "199 [D loss: 0.828832, acc.: 51.56%] [G loss: 1.543106]\n",
      "200 [D loss: 0.860868, acc.: 46.88%] [G loss: 1.299299]\n",
      "201 [D loss: 0.594243, acc.: 70.31%] [G loss: 1.416845]\n",
      "202 [D loss: 0.769184, acc.: 51.56%] [G loss: 1.124137]\n",
      "203 [D loss: 0.784674, acc.: 51.56%] [G loss: 1.197683]\n",
      "204 [D loss: 0.622157, acc.: 64.06%] [G loss: 1.004828]\n",
      "205 [D loss: 0.513088, acc.: 78.12%] [G loss: 1.475557]\n",
      "206 [D loss: 0.581368, acc.: 67.19%] [G loss: 1.080953]\n",
      "207 [D loss: 0.479974, acc.: 79.69%] [G loss: 1.204752]\n",
      "208 [D loss: 0.672519, acc.: 56.25%] [G loss: 0.869618]\n",
      "209 [D loss: 0.389956, acc.: 82.81%] [G loss: 1.191706]\n",
      "210 [D loss: 0.475875, acc.: 78.12%] [G loss: 1.154152]\n",
      "211 [D loss: 0.777133, acc.: 50.00%] [G loss: 0.842460]\n",
      "212 [D loss: 0.771672, acc.: 60.94%] [G loss: 1.090671]\n",
      "213 [D loss: 0.811939, acc.: 50.00%] [G loss: 1.580020]\n",
      "214 [D loss: 0.589022, acc.: 73.44%] [G loss: 1.341657]\n",
      "215 [D loss: 0.687495, acc.: 59.38%] [G loss: 1.158171]\n",
      "216 [D loss: 0.886658, acc.: 45.31%] [G loss: 1.138942]\n",
      "217 [D loss: 0.711126, acc.: 64.06%] [G loss: 1.188834]\n",
      "218 [D loss: 0.813791, acc.: 43.75%] [G loss: 1.381421]\n",
      "219 [D loss: 0.605664, acc.: 70.31%] [G loss: 1.552230]\n",
      "220 [D loss: 0.511984, acc.: 68.75%] [G loss: 1.097391]\n",
      "221 [D loss: 0.584750, acc.: 71.88%] [G loss: 1.148678]\n",
      "222 [D loss: 0.623612, acc.: 65.62%] [G loss: 1.069369]\n",
      "223 [D loss: 0.576371, acc.: 68.75%] [G loss: 1.271598]\n",
      "224 [D loss: 0.553425, acc.: 73.44%] [G loss: 1.339205]\n",
      "225 [D loss: 0.433162, acc.: 78.12%] [G loss: 1.040342]\n",
      "226 [D loss: 0.812384, acc.: 54.69%] [G loss: 0.847805]\n",
      "227 [D loss: 0.723358, acc.: 54.69%] [G loss: 0.766458]\n",
      "228 [D loss: 0.638735, acc.: 65.62%] [G loss: 1.076563]\n",
      "229 [D loss: 0.664370, acc.: 65.62%] [G loss: 0.839516]\n",
      "230 [D loss: 0.490614, acc.: 79.69%] [G loss: 0.790574]\n",
      "231 [D loss: 0.565598, acc.: 75.00%] [G loss: 1.552886]\n",
      "232 [D loss: 0.901929, acc.: 43.75%] [G loss: 1.390736]\n",
      "233 [D loss: 0.613968, acc.: 62.50%] [G loss: 0.993124]\n",
      "234 [D loss: 0.666723, acc.: 62.50%] [G loss: 0.680388]\n",
      "235 [D loss: 0.509051, acc.: 76.56%] [G loss: 1.016928]\n",
      "236 [D loss: 0.552009, acc.: 71.88%] [G loss: 0.934161]\n",
      "237 [D loss: 0.489242, acc.: 76.56%] [G loss: 0.984618]\n",
      "238 [D loss: 0.634516, acc.: 65.62%] [G loss: 1.000876]\n",
      "239 [D loss: 0.617287, acc.: 60.94%] [G loss: 1.432034]\n",
      "240 [D loss: 0.558086, acc.: 68.75%] [G loss: 0.784509]\n",
      "241 [D loss: 0.515912, acc.: 76.56%] [G loss: 0.993197]\n",
      "242 [D loss: 0.506234, acc.: 81.25%] [G loss: 1.119217]\n",
      "243 [D loss: 0.700379, acc.: 51.56%] [G loss: 1.270901]\n",
      "244 [D loss: 0.695963, acc.: 54.69%] [G loss: 0.670536]\n",
      "245 [D loss: 0.387392, acc.: 85.94%] [G loss: 0.615440]\n",
      "246 [D loss: 0.583921, acc.: 75.00%] [G loss: 0.849556]\n",
      "247 [D loss: 0.656450, acc.: 64.06%] [G loss: 1.065589]\n",
      "248 [D loss: 0.711576, acc.: 54.69%] [G loss: 0.915571]\n",
      "249 [D loss: 0.627132, acc.: 60.94%] [G loss: 1.147168]\n",
      "250 [D loss: 0.722079, acc.: 54.69%] [G loss: 1.121130]\n",
      "251 [D loss: 0.638418, acc.: 67.19%] [G loss: 0.758550]\n",
      "252 [D loss: 0.855756, acc.: 53.12%] [G loss: 0.826405]\n",
      "253 [D loss: 0.652826, acc.: 64.06%] [G loss: 0.939747]\n",
      "254 [D loss: 0.663825, acc.: 57.81%] [G loss: 1.127916]\n",
      "255 [D loss: 0.579295, acc.: 70.31%] [G loss: 0.904148]\n",
      "256 [D loss: 0.680289, acc.: 64.06%] [G loss: 0.740744]\n",
      "257 [D loss: 0.504979, acc.: 73.44%] [G loss: 1.105834]\n",
      "258 [D loss: 0.603478, acc.: 67.19%] [G loss: 1.124363]\n",
      "259 [D loss: 0.692031, acc.: 60.94%] [G loss: 1.142999]\n",
      "260 [D loss: 0.665952, acc.: 59.38%] [G loss: 0.756485]\n",
      "261 [D loss: 0.578946, acc.: 70.31%] [G loss: 1.202064]\n",
      "262 [D loss: 0.705950, acc.: 65.62%] [G loss: 1.193527]\n",
      "263 [D loss: 0.527702, acc.: 79.69%] [G loss: 1.338771]\n",
      "264 [D loss: 0.410907, acc.: 87.50%] [G loss: 1.083007]\n",
      "265 [D loss: 0.573504, acc.: 64.06%] [G loss: 0.966905]\n",
      "266 [D loss: 0.545412, acc.: 71.88%] [G loss: 0.826898]\n",
      "267 [D loss: 0.589764, acc.: 65.62%] [G loss: 0.844725]\n",
      "268 [D loss: 0.643021, acc.: 60.94%] [G loss: 0.960320]\n",
      "269 [D loss: 0.509523, acc.: 75.00%] [G loss: 1.198804]\n",
      "270 [D loss: 0.547340, acc.: 70.31%] [G loss: 1.131480]\n",
      "271 [D loss: 0.710316, acc.: 59.38%] [G loss: 1.312544]\n",
      "272 [D loss: 0.655254, acc.: 64.06%] [G loss: 1.396754]\n",
      "273 [D loss: 0.724953, acc.: 59.38%] [G loss: 1.179899]\n",
      "274 [D loss: 0.495860, acc.: 78.12%] [G loss: 1.033303]\n",
      "275 [D loss: 0.388727, acc.: 87.50%] [G loss: 1.117096]\n",
      "276 [D loss: 0.504936, acc.: 73.44%] [G loss: 1.382563]\n",
      "277 [D loss: 0.683328, acc.: 62.50%] [G loss: 0.875597]\n",
      "278 [D loss: 0.673136, acc.: 64.06%] [G loss: 1.260407]\n",
      "279 [D loss: 0.594944, acc.: 65.62%] [G loss: 1.121921]\n",
      "280 [D loss: 0.386104, acc.: 82.81%] [G loss: 1.318187]\n",
      "281 [D loss: 0.520586, acc.: 76.56%] [G loss: 1.280275]\n",
      "282 [D loss: 0.516395, acc.: 71.88%] [G loss: 1.176926]\n",
      "283 [D loss: 0.570521, acc.: 70.31%] [G loss: 0.876520]\n",
      "284 [D loss: 0.394983, acc.: 87.50%] [G loss: 0.730920]\n",
      "285 [D loss: 0.525775, acc.: 76.56%] [G loss: 0.802383]\n",
      "286 [D loss: 0.681442, acc.: 62.50%] [G loss: 0.710173]\n",
      "287 [D loss: 0.416246, acc.: 82.81%] [G loss: 1.158923]\n",
      "288 [D loss: 0.609834, acc.: 64.06%] [G loss: 1.079343]\n",
      "289 [D loss: 0.702737, acc.: 54.69%] [G loss: 1.043857]\n",
      "290 [D loss: 0.609117, acc.: 62.50%] [G loss: 1.309695]\n",
      "291 [D loss: 0.530432, acc.: 68.75%] [G loss: 1.268494]\n",
      "292 [D loss: 0.926483, acc.: 40.62%] [G loss: 0.953805]\n",
      "293 [D loss: 0.719556, acc.: 53.12%] [G loss: 1.641340]\n",
      "294 [D loss: 0.755893, acc.: 59.38%] [G loss: 1.274427]\n",
      "295 [D loss: 0.713289, acc.: 56.25%] [G loss: 0.922403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296 [D loss: 0.537458, acc.: 73.44%] [G loss: 1.390195]\n",
      "297 [D loss: 0.510333, acc.: 75.00%] [G loss: 1.226361]\n",
      "298 [D loss: 0.606100, acc.: 71.88%] [G loss: 1.353870]\n",
      "299 [D loss: 0.752768, acc.: 59.38%] [G loss: 1.009928]\n",
      "300 [D loss: 0.618196, acc.: 67.19%] [G loss: 1.237538]\n",
      "301 [D loss: 0.590253, acc.: 68.75%] [G loss: 1.432524]\n",
      "302 [D loss: 0.703005, acc.: 59.38%] [G loss: 1.172559]\n",
      "303 [D loss: 0.634137, acc.: 70.31%] [G loss: 1.266512]\n",
      "304 [D loss: 0.693260, acc.: 57.81%] [G loss: 0.851710]\n",
      "305 [D loss: 0.681150, acc.: 64.06%] [G loss: 1.284671]\n",
      "306 [D loss: 0.611970, acc.: 64.06%] [G loss: 0.777686]\n",
      "307 [D loss: 0.517174, acc.: 70.31%] [G loss: 1.320845]\n",
      "308 [D loss: 0.582522, acc.: 68.75%] [G loss: 1.165201]\n",
      "309 [D loss: 0.475131, acc.: 81.25%] [G loss: 1.317626]\n",
      "310 [D loss: 0.803466, acc.: 53.12%] [G loss: 1.064551]\n",
      "311 [D loss: 0.612799, acc.: 73.44%] [G loss: 0.862805]\n",
      "312 [D loss: 0.667424, acc.: 65.62%] [G loss: 1.316735]\n",
      "313 [D loss: 0.522669, acc.: 75.00%] [G loss: 1.455170]\n",
      "314 [D loss: 0.748408, acc.: 62.50%] [G loss: 1.226233]\n",
      "315 [D loss: 0.736333, acc.: 51.56%] [G loss: 0.984558]\n",
      "316 [D loss: 0.657463, acc.: 68.75%] [G loss: 0.830267]\n",
      "317 [D loss: 0.848063, acc.: 45.31%] [G loss: 1.054861]\n",
      "318 [D loss: 0.595730, acc.: 65.62%] [G loss: 1.321160]\n",
      "319 [D loss: 0.556208, acc.: 71.88%] [G loss: 1.609545]\n",
      "320 [D loss: 0.642116, acc.: 65.62%] [G loss: 1.406726]\n",
      "321 [D loss: 0.764493, acc.: 59.38%] [G loss: 0.747695]\n",
      "322 [D loss: 0.807945, acc.: 54.69%] [G loss: 1.211825]\n",
      "323 [D loss: 0.733566, acc.: 64.06%] [G loss: 1.479135]\n",
      "324 [D loss: 0.755090, acc.: 53.12%] [G loss: 1.462240]\n",
      "325 [D loss: 0.944336, acc.: 39.06%] [G loss: 1.412295]\n",
      "326 [D loss: 0.699642, acc.: 57.81%] [G loss: 1.450948]\n",
      "327 [D loss: 0.653136, acc.: 59.38%] [G loss: 1.336213]\n",
      "328 [D loss: 0.704951, acc.: 60.94%] [G loss: 1.446688]\n",
      "329 [D loss: 0.710092, acc.: 59.38%] [G loss: 1.365897]\n",
      "330 [D loss: 0.551661, acc.: 75.00%] [G loss: 1.367952]\n",
      "331 [D loss: 0.654969, acc.: 65.62%] [G loss: 1.034264]\n",
      "332 [D loss: 0.684696, acc.: 64.06%] [G loss: 1.391126]\n",
      "333 [D loss: 0.772369, acc.: 57.81%] [G loss: 1.124439]\n",
      "334 [D loss: 0.515293, acc.: 75.00%] [G loss: 0.945912]\n",
      "335 [D loss: 0.687323, acc.: 60.94%] [G loss: 1.156775]\n",
      "336 [D loss: 0.789698, acc.: 59.38%] [G loss: 1.207564]\n",
      "337 [D loss: 0.532825, acc.: 75.00%] [G loss: 1.045374]\n",
      "338 [D loss: 0.735120, acc.: 54.69%] [G loss: 0.984109]\n",
      "339 [D loss: 0.551252, acc.: 71.88%] [G loss: 1.189919]\n",
      "340 [D loss: 0.581958, acc.: 68.75%] [G loss: 1.205254]\n",
      "341 [D loss: 0.828392, acc.: 42.19%] [G loss: 1.107510]\n",
      "342 [D loss: 0.822235, acc.: 46.88%] [G loss: 1.002146]\n",
      "343 [D loss: 0.617822, acc.: 65.62%] [G loss: 1.022194]\n",
      "344 [D loss: 0.617465, acc.: 67.19%] [G loss: 1.259693]\n",
      "345 [D loss: 0.794599, acc.: 48.44%] [G loss: 1.278815]\n",
      "346 [D loss: 0.613696, acc.: 68.75%] [G loss: 1.500024]\n",
      "347 [D loss: 0.666439, acc.: 68.75%] [G loss: 1.393535]\n",
      "348 [D loss: 0.654457, acc.: 62.50%] [G loss: 1.260741]\n",
      "349 [D loss: 0.565251, acc.: 75.00%] [G loss: 1.362442]\n",
      "350 [D loss: 0.643124, acc.: 62.50%] [G loss: 1.125776]\n",
      "351 [D loss: 0.703411, acc.: 60.94%] [G loss: 1.322058]\n",
      "352 [D loss: 0.723481, acc.: 56.25%] [G loss: 1.884369]\n",
      "353 [D loss: 0.569318, acc.: 71.88%] [G loss: 1.562066]\n",
      "354 [D loss: 0.698560, acc.: 57.81%] [G loss: 1.148008]\n",
      "355 [D loss: 0.504848, acc.: 71.88%] [G loss: 1.055222]\n",
      "356 [D loss: 0.698152, acc.: 59.38%] [G loss: 1.118859]\n",
      "357 [D loss: 0.625747, acc.: 64.06%] [G loss: 1.455154]\n",
      "358 [D loss: 0.719229, acc.: 56.25%] [G loss: 1.442214]\n",
      "359 [D loss: 0.597558, acc.: 68.75%] [G loss: 1.281349]\n",
      "360 [D loss: 0.740251, acc.: 62.50%] [G loss: 1.127962]\n",
      "361 [D loss: 0.778481, acc.: 53.12%] [G loss: 1.365831]\n",
      "362 [D loss: 0.742286, acc.: 50.00%] [G loss: 1.441228]\n",
      "363 [D loss: 0.688997, acc.: 65.62%] [G loss: 0.981621]\n",
      "364 [D loss: 0.667335, acc.: 62.50%] [G loss: 1.209276]\n",
      "365 [D loss: 0.564632, acc.: 76.56%] [G loss: 1.097675]\n",
      "366 [D loss: 0.652508, acc.: 65.62%] [G loss: 1.463826]\n",
      "367 [D loss: 0.845607, acc.: 51.56%] [G loss: 0.826873]\n",
      "368 [D loss: 0.667505, acc.: 57.81%] [G loss: 1.217594]\n",
      "369 [D loss: 0.635701, acc.: 64.06%] [G loss: 1.344208]\n",
      "370 [D loss: 0.659768, acc.: 65.62%] [G loss: 1.235440]\n",
      "371 [D loss: 0.674745, acc.: 60.94%] [G loss: 1.239636]\n",
      "372 [D loss: 0.763629, acc.: 60.94%] [G loss: 0.759278]\n",
      "373 [D loss: 0.673669, acc.: 56.25%] [G loss: 0.778625]\n",
      "374 [D loss: 0.621285, acc.: 70.31%] [G loss: 0.918586]\n",
      "375 [D loss: 0.687029, acc.: 62.50%] [G loss: 1.105958]\n",
      "376 [D loss: 0.656522, acc.: 65.62%] [G loss: 1.306613]\n",
      "377 [D loss: 0.766443, acc.: 54.69%] [G loss: 1.190076]\n",
      "378 [D loss: 0.612846, acc.: 67.19%] [G loss: 1.292771]\n",
      "379 [D loss: 0.670525, acc.: 60.94%] [G loss: 1.241254]\n",
      "380 [D loss: 0.568511, acc.: 71.88%] [G loss: 1.131786]\n",
      "381 [D loss: 0.991494, acc.: 35.94%] [G loss: 0.934723]\n",
      "382 [D loss: 0.851284, acc.: 51.56%] [G loss: 1.135641]\n",
      "383 [D loss: 0.561043, acc.: 73.44%] [G loss: 1.334603]\n",
      "384 [D loss: 0.679509, acc.: 59.38%] [G loss: 1.418512]\n",
      "385 [D loss: 0.557473, acc.: 67.19%] [G loss: 1.260215]\n",
      "386 [D loss: 0.461171, acc.: 79.69%] [G loss: 1.165608]\n",
      "387 [D loss: 0.715947, acc.: 57.81%] [G loss: 1.345960]\n",
      "388 [D loss: 0.587234, acc.: 68.75%] [G loss: 1.256675]\n",
      "389 [D loss: 0.764866, acc.: 56.25%] [G loss: 1.114174]\n",
      "390 [D loss: 0.568234, acc.: 67.19%] [G loss: 1.379628]\n",
      "391 [D loss: 0.593147, acc.: 64.06%] [G loss: 1.256490]\n",
      "392 [D loss: 0.651029, acc.: 70.31%] [G loss: 1.230278]\n",
      "393 [D loss: 0.666490, acc.: 59.38%] [G loss: 1.571366]\n",
      "394 [D loss: 0.712963, acc.: 59.38%] [G loss: 1.465281]\n",
      "395 [D loss: 0.760990, acc.: 62.50%] [G loss: 1.622585]\n",
      "396 [D loss: 0.752694, acc.: 51.56%] [G loss: 1.536022]\n",
      "397 [D loss: 0.585976, acc.: 68.75%] [G loss: 1.477864]\n",
      "398 [D loss: 0.696785, acc.: 57.81%] [G loss: 1.266127]\n",
      "399 [D loss: 0.802199, acc.: 48.44%] [G loss: 1.250080]\n",
      "400 [D loss: 0.715294, acc.: 54.69%] [G loss: 1.450974]\n",
      "401 [D loss: 0.660730, acc.: 62.50%] [G loss: 0.911262]\n",
      "402 [D loss: 0.695388, acc.: 64.06%] [G loss: 1.398283]\n",
      "403 [D loss: 0.558751, acc.: 67.19%] [G loss: 1.106628]\n",
      "404 [D loss: 0.518562, acc.: 76.56%] [G loss: 0.868700]\n",
      "405 [D loss: 0.501756, acc.: 84.38%] [G loss: 0.849261]\n",
      "406 [D loss: 0.523165, acc.: 76.56%] [G loss: 0.846880]\n",
      "407 [D loss: 0.463938, acc.: 84.38%] [G loss: 1.342406]\n",
      "408 [D loss: 0.566763, acc.: 65.62%] [G loss: 1.100748]\n",
      "409 [D loss: 0.682880, acc.: 67.19%] [G loss: 1.104879]\n",
      "410 [D loss: 0.581728, acc.: 68.75%] [G loss: 1.059047]\n",
      "411 [D loss: 0.696465, acc.: 54.69%] [G loss: 1.084786]\n",
      "412 [D loss: 0.618549, acc.: 59.38%] [G loss: 1.286603]\n",
      "413 [D loss: 0.628471, acc.: 64.06%] [G loss: 1.435152]\n",
      "414 [D loss: 0.740112, acc.: 57.81%] [G loss: 1.185861]\n",
      "415 [D loss: 0.719649, acc.: 60.94%] [G loss: 1.187938]\n",
      "416 [D loss: 0.768145, acc.: 59.38%] [G loss: 1.072052]\n",
      "417 [D loss: 0.892297, acc.: 34.38%] [G loss: 1.135145]\n",
      "418 [D loss: 0.659586, acc.: 64.06%] [G loss: 1.506103]\n",
      "419 [D loss: 0.564504, acc.: 68.75%] [G loss: 1.503110]\n",
      "420 [D loss: 0.768276, acc.: 57.81%] [G loss: 1.408522]\n",
      "421 [D loss: 0.679122, acc.: 64.06%] [G loss: 1.172519]\n",
      "422 [D loss: 0.640057, acc.: 60.94%] [G loss: 1.337941]\n",
      "423 [D loss: 0.444444, acc.: 78.12%] [G loss: 1.440040]\n",
      "424 [D loss: 0.526827, acc.: 71.88%] [G loss: 1.383970]\n",
      "425 [D loss: 0.587484, acc.: 75.00%] [G loss: 1.584179]\n",
      "426 [D loss: 0.524511, acc.: 79.69%] [G loss: 1.238666]\n",
      "427 [D loss: 0.641242, acc.: 59.38%] [G loss: 1.241552]\n",
      "428 [D loss: 0.698375, acc.: 59.38%] [G loss: 0.915105]\n",
      "429 [D loss: 0.657023, acc.: 60.94%] [G loss: 1.152360]\n",
      "430 [D loss: 0.496961, acc.: 78.12%] [G loss: 1.049171]\n",
      "431 [D loss: 0.546782, acc.: 70.31%] [G loss: 1.154598]\n",
      "432 [D loss: 0.622978, acc.: 59.38%] [G loss: 0.777329]\n",
      "433 [D loss: 0.800349, acc.: 53.12%] [G loss: 1.265755]\n",
      "434 [D loss: 0.629773, acc.: 62.50%] [G loss: 0.986648]\n",
      "435 [D loss: 0.814983, acc.: 48.44%] [G loss: 1.391274]\n",
      "436 [D loss: 0.830807, acc.: 50.00%] [G loss: 1.174576]\n",
      "437 [D loss: 0.729147, acc.: 53.12%] [G loss: 1.530398]\n",
      "438 [D loss: 0.726169, acc.: 50.00%] [G loss: 1.699570]\n",
      "439 [D loss: 0.799779, acc.: 59.38%] [G loss: 1.294074]\n",
      "440 [D loss: 0.674356, acc.: 64.06%] [G loss: 1.655897]\n",
      "441 [D loss: 0.606812, acc.: 73.44%] [G loss: 1.255959]\n",
      "442 [D loss: 0.733872, acc.: 56.25%] [G loss: 1.270169]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443 [D loss: 0.649835, acc.: 67.19%] [G loss: 1.515904]\n",
      "444 [D loss: 0.681306, acc.: 64.06%] [G loss: 1.306240]\n",
      "445 [D loss: 0.775018, acc.: 50.00%] [G loss: 1.239265]\n",
      "446 [D loss: 0.623996, acc.: 60.94%] [G loss: 1.219285]\n",
      "447 [D loss: 0.738185, acc.: 46.88%] [G loss: 1.389983]\n",
      "448 [D loss: 0.587523, acc.: 62.50%] [G loss: 1.055153]\n",
      "449 [D loss: 0.565991, acc.: 75.00%] [G loss: 1.406944]\n",
      "450 [D loss: 0.671331, acc.: 60.94%] [G loss: 1.215621]\n",
      "451 [D loss: 0.709708, acc.: 59.38%] [G loss: 1.482581]\n",
      "452 [D loss: 0.699171, acc.: 59.38%] [G loss: 1.691165]\n",
      "453 [D loss: 0.613617, acc.: 64.06%] [G loss: 0.952431]\n",
      "454 [D loss: 0.784094, acc.: 51.56%] [G loss: 0.742230]\n",
      "455 [D loss: 0.548663, acc.: 76.56%] [G loss: 1.161663]\n",
      "456 [D loss: 0.669638, acc.: 60.94%] [G loss: 1.072115]\n",
      "457 [D loss: 0.659335, acc.: 60.94%] [G loss: 1.064096]\n",
      "458 [D loss: 0.737049, acc.: 50.00%] [G loss: 1.417575]\n",
      "459 [D loss: 0.563953, acc.: 70.31%] [G loss: 1.662233]\n",
      "460 [D loss: 0.573997, acc.: 65.62%] [G loss: 1.102744]\n",
      "461 [D loss: 0.667855, acc.: 65.62%] [G loss: 1.225831]\n",
      "462 [D loss: 0.593688, acc.: 67.19%] [G loss: 1.168997]\n",
      "463 [D loss: 0.610767, acc.: 65.62%] [G loss: 1.357416]\n",
      "464 [D loss: 0.600822, acc.: 68.75%] [G loss: 1.029258]\n",
      "465 [D loss: 0.831442, acc.: 46.88%] [G loss: 1.279922]\n",
      "466 [D loss: 0.710058, acc.: 56.25%] [G loss: 1.408742]\n",
      "467 [D loss: 0.713380, acc.: 62.50%] [G loss: 1.258913]\n",
      "468 [D loss: 0.742045, acc.: 60.94%] [G loss: 1.396000]\n",
      "469 [D loss: 0.744731, acc.: 54.69%] [G loss: 1.210208]\n",
      "470 [D loss: 0.998017, acc.: 37.50%] [G loss: 1.110225]\n",
      "471 [D loss: 0.650371, acc.: 60.94%] [G loss: 1.464427]\n",
      "472 [D loss: 0.576671, acc.: 64.06%] [G loss: 1.474403]\n",
      "473 [D loss: 0.673724, acc.: 62.50%] [G loss: 1.570909]\n",
      "474 [D loss: 0.640195, acc.: 67.19%] [G loss: 0.944101]\n",
      "475 [D loss: 0.538272, acc.: 68.75%] [G loss: 1.194278]\n",
      "476 [D loss: 0.668380, acc.: 62.50%] [G loss: 1.700832]\n",
      "477 [D loss: 0.696456, acc.: 59.38%] [G loss: 1.543342]\n",
      "478 [D loss: 0.735032, acc.: 51.56%] [G loss: 1.336787]\n",
      "479 [D loss: 0.642285, acc.: 62.50%] [G loss: 1.328358]\n",
      "480 [D loss: 0.666178, acc.: 54.69%] [G loss: 1.064587]\n",
      "481 [D loss: 0.670422, acc.: 57.81%] [G loss: 1.314982]\n",
      "482 [D loss: 0.663195, acc.: 65.62%] [G loss: 1.599086]\n",
      "483 [D loss: 0.633181, acc.: 68.75%] [G loss: 1.257250]\n",
      "484 [D loss: 0.513231, acc.: 78.12%] [G loss: 1.173286]\n",
      "485 [D loss: 0.703611, acc.: 59.38%] [G loss: 1.556430]\n",
      "486 [D loss: 0.589977, acc.: 71.88%] [G loss: 1.232554]\n",
      "487 [D loss: 0.738964, acc.: 56.25%] [G loss: 0.955990]\n",
      "488 [D loss: 0.579983, acc.: 76.56%] [G loss: 1.113780]\n",
      "489 [D loss: 0.737713, acc.: 56.25%] [G loss: 1.057073]\n",
      "490 [D loss: 0.578330, acc.: 64.06%] [G loss: 1.098184]\n",
      "491 [D loss: 0.677947, acc.: 60.94%] [G loss: 1.450377]\n",
      "492 [D loss: 0.692472, acc.: 51.56%] [G loss: 1.100451]\n",
      "493 [D loss: 0.731866, acc.: 56.25%] [G loss: 0.950977]\n",
      "494 [D loss: 0.750796, acc.: 56.25%] [G loss: 1.222883]\n",
      "495 [D loss: 0.599960, acc.: 67.19%] [G loss: 1.350870]\n",
      "496 [D loss: 0.722714, acc.: 53.12%] [G loss: 1.229855]\n",
      "497 [D loss: 0.526146, acc.: 73.44%] [G loss: 1.254842]\n",
      "498 [D loss: 0.681234, acc.: 65.62%] [G loss: 1.549467]\n",
      "499 [D loss: 0.840848, acc.: 46.88%] [G loss: 1.481962]\n",
      "500 [D loss: 0.535861, acc.: 68.75%] [G loss: 1.519895]\n",
      "501 [D loss: 0.565954, acc.: 70.31%] [G loss: 1.471110]\n",
      "502 [D loss: 0.637641, acc.: 62.50%] [G loss: 1.121163]\n",
      "503 [D loss: 0.710767, acc.: 53.12%] [G loss: 1.232743]\n",
      "504 [D loss: 0.681576, acc.: 65.62%] [G loss: 1.454269]\n",
      "505 [D loss: 0.621782, acc.: 71.88%] [G loss: 1.305830]\n",
      "506 [D loss: 0.831604, acc.: 42.19%] [G loss: 1.214613]\n",
      "507 [D loss: 0.623731, acc.: 62.50%] [G loss: 1.406051]\n",
      "508 [D loss: 0.744570, acc.: 54.69%] [G loss: 1.211349]\n",
      "509 [D loss: 0.641441, acc.: 67.19%] [G loss: 0.982311]\n",
      "510 [D loss: 0.480154, acc.: 79.69%] [G loss: 1.173687]\n",
      "511 [D loss: 0.764583, acc.: 48.44%] [G loss: 1.140506]\n",
      "512 [D loss: 0.742863, acc.: 51.56%] [G loss: 1.204582]\n",
      "513 [D loss: 0.658357, acc.: 62.50%] [G loss: 1.293789]\n",
      "514 [D loss: 0.715243, acc.: 48.44%] [G loss: 1.116259]\n",
      "515 [D loss: 0.703974, acc.: 60.94%] [G loss: 1.724136]\n",
      "516 [D loss: 0.855511, acc.: 39.06%] [G loss: 1.016387]\n",
      "517 [D loss: 0.605170, acc.: 68.75%] [G loss: 1.498399]\n",
      "518 [D loss: 0.689464, acc.: 59.38%] [G loss: 1.393162]\n",
      "519 [D loss: 0.726359, acc.: 60.94%] [G loss: 0.995801]\n",
      "520 [D loss: 0.856195, acc.: 50.00%] [G loss: 1.009536]\n",
      "521 [D loss: 0.676987, acc.: 59.38%] [G loss: 1.238159]\n",
      "522 [D loss: 0.656785, acc.: 64.06%] [G loss: 1.189328]\n",
      "523 [D loss: 0.610032, acc.: 65.62%] [G loss: 1.036873]\n",
      "524 [D loss: 0.597143, acc.: 68.75%] [G loss: 0.973970]\n",
      "525 [D loss: 0.688474, acc.: 57.81%] [G loss: 1.336075]\n",
      "526 [D loss: 0.632067, acc.: 64.06%] [G loss: 1.379683]\n",
      "527 [D loss: 0.578002, acc.: 67.19%] [G loss: 1.444412]\n",
      "528 [D loss: 0.646719, acc.: 65.62%] [G loss: 1.384369]\n",
      "529 [D loss: 0.816540, acc.: 50.00%] [G loss: 1.186291]\n",
      "530 [D loss: 0.655233, acc.: 64.06%] [G loss: 1.528565]\n",
      "531 [D loss: 0.642708, acc.: 62.50%] [G loss: 1.642733]\n",
      "532 [D loss: 0.782351, acc.: 54.69%] [G loss: 1.156663]\n",
      "533 [D loss: 0.782289, acc.: 54.69%] [G loss: 1.192455]\n",
      "534 [D loss: 0.527477, acc.: 78.12%] [G loss: 1.129973]\n",
      "535 [D loss: 0.752630, acc.: 54.69%] [G loss: 1.133653]\n",
      "536 [D loss: 0.675855, acc.: 57.81%] [G loss: 1.466414]\n",
      "537 [D loss: 0.759304, acc.: 51.56%] [G loss: 1.021085]\n",
      "538 [D loss: 0.690907, acc.: 56.25%] [G loss: 1.146052]\n",
      "539 [D loss: 0.613422, acc.: 65.62%] [G loss: 1.224036]\n",
      "540 [D loss: 0.545491, acc.: 73.44%] [G loss: 1.163153]\n",
      "541 [D loss: 0.742453, acc.: 51.56%] [G loss: 0.907518]\n",
      "542 [D loss: 0.658252, acc.: 62.50%] [G loss: 1.206721]\n",
      "543 [D loss: 0.767766, acc.: 50.00%] [G loss: 1.171655]\n",
      "544 [D loss: 0.692554, acc.: 59.38%] [G loss: 1.505432]\n",
      "545 [D loss: 0.614121, acc.: 67.19%] [G loss: 1.096260]\n",
      "546 [D loss: 0.598280, acc.: 62.50%] [G loss: 1.090358]\n",
      "547 [D loss: 0.865369, acc.: 48.44%] [G loss: 0.885589]\n",
      "548 [D loss: 0.616536, acc.: 65.62%] [G loss: 1.303851]\n",
      "549 [D loss: 0.746722, acc.: 57.81%] [G loss: 1.177329]\n",
      "550 [D loss: 0.651796, acc.: 67.19%] [G loss: 1.361465]\n",
      "551 [D loss: 0.574084, acc.: 70.31%] [G loss: 1.333251]\n",
      "552 [D loss: 0.612266, acc.: 70.31%] [G loss: 1.322440]\n",
      "553 [D loss: 0.554697, acc.: 67.19%] [G loss: 1.204566]\n",
      "554 [D loss: 0.669069, acc.: 73.44%] [G loss: 1.105511]\n",
      "555 [D loss: 0.515730, acc.: 75.00%] [G loss: 1.337087]\n",
      "556 [D loss: 0.587329, acc.: 70.31%] [G loss: 1.584497]\n",
      "557 [D loss: 0.645225, acc.: 64.06%] [G loss: 1.256019]\n",
      "558 [D loss: 0.714886, acc.: 50.00%] [G loss: 1.363003]\n",
      "559 [D loss: 0.674052, acc.: 56.25%] [G loss: 1.206173]\n",
      "560 [D loss: 0.557511, acc.: 71.88%] [G loss: 1.493965]\n",
      "561 [D loss: 0.735628, acc.: 54.69%] [G loss: 1.189727]\n",
      "562 [D loss: 0.579111, acc.: 64.06%] [G loss: 1.443283]\n",
      "563 [D loss: 0.809216, acc.: 53.12%] [G loss: 1.154149]\n",
      "564 [D loss: 0.492731, acc.: 78.12%] [G loss: 1.612187]\n",
      "565 [D loss: 0.633797, acc.: 67.19%] [G loss: 1.540628]\n",
      "566 [D loss: 0.751145, acc.: 56.25%] [G loss: 1.221381]\n",
      "567 [D loss: 0.536238, acc.: 75.00%] [G loss: 1.713034]\n",
      "568 [D loss: 0.665811, acc.: 62.50%] [G loss: 0.913661]\n",
      "569 [D loss: 0.590689, acc.: 68.75%] [G loss: 1.225363]\n",
      "570 [D loss: 0.665365, acc.: 60.94%] [G loss: 1.099366]\n",
      "571 [D loss: 0.709892, acc.: 56.25%] [G loss: 1.343336]\n",
      "572 [D loss: 0.635418, acc.: 60.94%] [G loss: 1.707959]\n",
      "573 [D loss: 0.726327, acc.: 59.38%] [G loss: 1.481703]\n",
      "574 [D loss: 0.857733, acc.: 45.31%] [G loss: 1.259644]\n",
      "575 [D loss: 0.677271, acc.: 57.81%] [G loss: 1.498460]\n",
      "576 [D loss: 0.600875, acc.: 68.75%] [G loss: 1.447461]\n",
      "577 [D loss: 0.684210, acc.: 56.25%] [G loss: 1.015324]\n",
      "578 [D loss: 0.599221, acc.: 64.06%] [G loss: 1.287522]\n",
      "579 [D loss: 0.642776, acc.: 62.50%] [G loss: 1.423382]\n",
      "580 [D loss: 0.643666, acc.: 62.50%] [G loss: 1.196744]\n",
      "581 [D loss: 0.726762, acc.: 48.44%] [G loss: 1.152323]\n",
      "582 [D loss: 0.678879, acc.: 57.81%] [G loss: 1.087004]\n",
      "583 [D loss: 0.610260, acc.: 75.00%] [G loss: 1.279433]\n",
      "584 [D loss: 0.506537, acc.: 71.88%] [G loss: 1.302198]\n",
      "585 [D loss: 0.679809, acc.: 60.94%] [G loss: 1.508532]\n",
      "586 [D loss: 0.666259, acc.: 57.81%] [G loss: 1.335955]\n",
      "587 [D loss: 0.669348, acc.: 57.81%] [G loss: 1.474760]\n",
      "588 [D loss: 0.665932, acc.: 57.81%] [G loss: 1.227455]\n",
      "589 [D loss: 0.940604, acc.: 40.62%] [G loss: 1.066149]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "590 [D loss: 0.824535, acc.: 42.19%] [G loss: 0.973258]\n",
      "591 [D loss: 0.718522, acc.: 53.12%] [G loss: 1.205039]\n",
      "592 [D loss: 0.635858, acc.: 60.94%] [G loss: 1.342376]\n",
      "593 [D loss: 0.588781, acc.: 71.88%] [G loss: 1.178331]\n",
      "594 [D loss: 0.620314, acc.: 65.62%] [G loss: 1.026330]\n",
      "595 [D loss: 0.747616, acc.: 50.00%] [G loss: 0.963321]\n",
      "596 [D loss: 0.644777, acc.: 64.06%] [G loss: 1.375597]\n",
      "597 [D loss: 0.724878, acc.: 53.12%] [G loss: 1.233688]\n",
      "598 [D loss: 0.746554, acc.: 53.12%] [G loss: 1.343006]\n",
      "599 [D loss: 0.780294, acc.: 50.00%] [G loss: 1.053215]\n",
      "600 [D loss: 0.652195, acc.: 57.81%] [G loss: 1.414881]\n",
      "601 [D loss: 0.755727, acc.: 50.00%] [G loss: 1.079149]\n",
      "602 [D loss: 0.770301, acc.: 53.12%] [G loss: 1.055689]\n",
      "603 [D loss: 0.664509, acc.: 67.19%] [G loss: 1.059531]\n",
      "604 [D loss: 0.610502, acc.: 59.38%] [G loss: 1.410954]\n",
      "605 [D loss: 0.577749, acc.: 70.31%] [G loss: 1.402313]\n",
      "606 [D loss: 0.746319, acc.: 57.81%] [G loss: 1.365547]\n",
      "607 [D loss: 0.838224, acc.: 43.75%] [G loss: 1.483793]\n",
      "608 [D loss: 0.735144, acc.: 51.56%] [G loss: 1.487229]\n",
      "609 [D loss: 0.589747, acc.: 68.75%] [G loss: 1.288164]\n",
      "610 [D loss: 0.918326, acc.: 40.62%] [G loss: 1.075554]\n",
      "611 [D loss: 0.654428, acc.: 62.50%] [G loss: 1.409085]\n",
      "612 [D loss: 0.711287, acc.: 60.94%] [G loss: 1.096798]\n",
      "613 [D loss: 0.759767, acc.: 56.25%] [G loss: 1.275874]\n",
      "614 [D loss: 0.615231, acc.: 62.50%] [G loss: 1.051147]\n",
      "615 [D loss: 0.757693, acc.: 53.12%] [G loss: 1.281096]\n",
      "616 [D loss: 0.618773, acc.: 68.75%] [G loss: 1.300158]\n",
      "617 [D loss: 0.735788, acc.: 60.94%] [G loss: 1.460044]\n",
      "618 [D loss: 0.724489, acc.: 56.25%] [G loss: 1.155917]\n",
      "619 [D loss: 0.708589, acc.: 56.25%] [G loss: 1.326561]\n",
      "620 [D loss: 0.790130, acc.: 57.81%] [G loss: 1.430650]\n",
      "621 [D loss: 0.731988, acc.: 48.44%] [G loss: 1.221801]\n",
      "622 [D loss: 0.713830, acc.: 56.25%] [G loss: 1.085190]\n",
      "623 [D loss: 0.585283, acc.: 67.19%] [G loss: 1.201865]\n",
      "624 [D loss: 0.794903, acc.: 53.12%] [G loss: 1.079183]\n",
      "625 [D loss: 0.708262, acc.: 54.69%] [G loss: 1.167823]\n",
      "626 [D loss: 0.717885, acc.: 57.81%] [G loss: 1.072560]\n",
      "627 [D loss: 0.597715, acc.: 65.62%] [G loss: 1.262138]\n",
      "628 [D loss: 0.696201, acc.: 57.81%] [G loss: 1.238234]\n",
      "629 [D loss: 0.716970, acc.: 57.81%] [G loss: 1.195898]\n",
      "630 [D loss: 0.634394, acc.: 59.38%] [G loss: 1.360767]\n",
      "631 [D loss: 0.670618, acc.: 65.62%] [G loss: 1.171261]\n",
      "632 [D loss: 0.626652, acc.: 65.62%] [G loss: 1.379586]\n",
      "633 [D loss: 0.675322, acc.: 59.38%] [G loss: 1.132691]\n",
      "634 [D loss: 0.567845, acc.: 65.62%] [G loss: 1.218137]\n",
      "635 [D loss: 0.642163, acc.: 67.19%] [G loss: 1.212680]\n",
      "636 [D loss: 0.760976, acc.: 57.81%] [G loss: 1.292601]\n",
      "637 [D loss: 0.802703, acc.: 48.44%] [G loss: 1.080500]\n",
      "638 [D loss: 0.687402, acc.: 64.06%] [G loss: 1.277669]\n",
      "639 [D loss: 0.667939, acc.: 54.69%] [G loss: 1.157667]\n",
      "640 [D loss: 0.650727, acc.: 57.81%] [G loss: 1.251902]\n",
      "641 [D loss: 0.687538, acc.: 60.94%] [G loss: 1.219562]\n",
      "642 [D loss: 0.721850, acc.: 54.69%] [G loss: 1.141108]\n",
      "643 [D loss: 0.752588, acc.: 53.12%] [G loss: 1.148277]\n",
      "644 [D loss: 0.641280, acc.: 65.62%] [G loss: 1.492139]\n",
      "645 [D loss: 0.685923, acc.: 65.62%] [G loss: 1.100593]\n",
      "646 [D loss: 0.759167, acc.: 51.56%] [G loss: 1.079733]\n",
      "647 [D loss: 0.538149, acc.: 71.88%] [G loss: 1.054250]\n",
      "648 [D loss: 0.762971, acc.: 59.38%] [G loss: 1.207873]\n",
      "649 [D loss: 0.643686, acc.: 68.75%] [G loss: 1.167528]\n",
      "650 [D loss: 0.600546, acc.: 62.50%] [G loss: 1.377964]\n",
      "651 [D loss: 0.668190, acc.: 64.06%] [G loss: 1.440554]\n",
      "652 [D loss: 0.767511, acc.: 53.12%] [G loss: 0.994975]\n",
      "653 [D loss: 0.564036, acc.: 75.00%] [G loss: 0.903822]\n",
      "654 [D loss: 0.679325, acc.: 62.50%] [G loss: 1.194018]\n",
      "655 [D loss: 0.846078, acc.: 43.75%] [G loss: 1.191937]\n",
      "656 [D loss: 0.697837, acc.: 56.25%] [G loss: 1.240052]\n",
      "657 [D loss: 0.595466, acc.: 70.31%] [G loss: 1.267889]\n",
      "658 [D loss: 0.624696, acc.: 67.19%] [G loss: 1.105352]\n",
      "659 [D loss: 0.542494, acc.: 73.44%] [G loss: 1.351420]\n",
      "660 [D loss: 0.653484, acc.: 59.38%] [G loss: 1.334050]\n",
      "661 [D loss: 0.610442, acc.: 65.62%] [G loss: 1.216804]\n",
      "662 [D loss: 0.649979, acc.: 59.38%] [G loss: 1.267057]\n",
      "663 [D loss: 0.601826, acc.: 67.19%] [G loss: 1.038936]\n",
      "664 [D loss: 0.667772, acc.: 59.38%] [G loss: 1.156003]\n",
      "665 [D loss: 0.718862, acc.: 48.44%] [G loss: 1.030112]\n",
      "666 [D loss: 0.675429, acc.: 57.81%] [G loss: 0.938463]\n",
      "667 [D loss: 0.708347, acc.: 54.69%] [G loss: 0.952110]\n",
      "668 [D loss: 0.742736, acc.: 56.25%] [G loss: 1.173053]\n",
      "669 [D loss: 0.702811, acc.: 57.81%] [G loss: 1.040668]\n",
      "670 [D loss: 0.679450, acc.: 64.06%] [G loss: 1.086341]\n",
      "671 [D loss: 0.886904, acc.: 42.19%] [G loss: 1.341202]\n",
      "672 [D loss: 0.708420, acc.: 51.56%] [G loss: 1.454762]\n",
      "673 [D loss: 0.762417, acc.: 51.56%] [G loss: 1.337754]\n",
      "674 [D loss: 0.687496, acc.: 60.94%] [G loss: 1.486871]\n",
      "675 [D loss: 0.589871, acc.: 65.62%] [G loss: 1.455203]\n",
      "676 [D loss: 0.710912, acc.: 50.00%] [G loss: 1.579054]\n",
      "677 [D loss: 0.635146, acc.: 59.38%] [G loss: 1.432700]\n",
      "678 [D loss: 0.764144, acc.: 51.56%] [G loss: 1.281272]\n",
      "679 [D loss: 0.605060, acc.: 65.62%] [G loss: 1.233481]\n",
      "680 [D loss: 0.783252, acc.: 57.81%] [G loss: 1.069747]\n",
      "681 [D loss: 0.673592, acc.: 62.50%] [G loss: 1.336308]\n",
      "682 [D loss: 0.720742, acc.: 51.56%] [G loss: 1.199831]\n",
      "683 [D loss: 0.596002, acc.: 73.44%] [G loss: 1.359873]\n",
      "684 [D loss: 0.637123, acc.: 59.38%] [G loss: 1.139693]\n",
      "685 [D loss: 0.804681, acc.: 46.88%] [G loss: 1.017262]\n",
      "686 [D loss: 0.744128, acc.: 51.56%] [G loss: 1.211231]\n",
      "687 [D loss: 0.693893, acc.: 57.81%] [G loss: 1.041036]\n",
      "688 [D loss: 0.652751, acc.: 59.38%] [G loss: 1.325705]\n",
      "689 [D loss: 0.692203, acc.: 57.81%] [G loss: 1.344057]\n",
      "690 [D loss: 0.780606, acc.: 51.56%] [G loss: 1.164812]\n",
      "691 [D loss: 0.739268, acc.: 57.81%] [G loss: 1.163663]\n",
      "692 [D loss: 0.673628, acc.: 59.38%] [G loss: 1.120400]\n",
      "693 [D loss: 0.582055, acc.: 65.62%] [G loss: 1.178733]\n",
      "694 [D loss: 0.928937, acc.: 43.75%] [G loss: 0.945200]\n",
      "695 [D loss: 0.908908, acc.: 40.62%] [G loss: 1.083027]\n",
      "696 [D loss: 0.641832, acc.: 60.94%] [G loss: 1.160195]\n",
      "697 [D loss: 0.597046, acc.: 68.75%] [G loss: 1.334726]\n",
      "698 [D loss: 0.682853, acc.: 59.38%] [G loss: 1.203434]\n",
      "699 [D loss: 0.716472, acc.: 56.25%] [G loss: 1.083905]\n",
      "700 [D loss: 0.741626, acc.: 53.12%] [G loss: 1.139824]\n",
      "701 [D loss: 0.735445, acc.: 56.25%] [G loss: 1.056716]\n",
      "702 [D loss: 0.642163, acc.: 64.06%] [G loss: 1.182588]\n",
      "703 [D loss: 0.574211, acc.: 67.19%] [G loss: 1.348728]\n",
      "704 [D loss: 0.832671, acc.: 39.06%] [G loss: 0.880896]\n",
      "705 [D loss: 0.678702, acc.: 50.00%] [G loss: 1.034065]\n",
      "706 [D loss: 0.726803, acc.: 56.25%] [G loss: 1.143184]\n",
      "707 [D loss: 0.892969, acc.: 39.06%] [G loss: 0.992702]\n",
      "708 [D loss: 0.685594, acc.: 59.38%] [G loss: 1.268395]\n",
      "709 [D loss: 0.576290, acc.: 73.44%] [G loss: 1.367188]\n",
      "710 [D loss: 0.838973, acc.: 51.56%] [G loss: 1.020905]\n",
      "711 [D loss: 0.700045, acc.: 65.62%] [G loss: 1.197785]\n",
      "712 [D loss: 0.716913, acc.: 53.12%] [G loss: 1.183371]\n",
      "713 [D loss: 0.586707, acc.: 67.19%] [G loss: 1.151410]\n",
      "714 [D loss: 0.671095, acc.: 54.69%] [G loss: 1.052690]\n",
      "715 [D loss: 0.581463, acc.: 75.00%] [G loss: 1.073969]\n",
      "716 [D loss: 0.649765, acc.: 62.50%] [G loss: 1.217211]\n",
      "717 [D loss: 0.743844, acc.: 51.56%] [G loss: 1.041254]\n",
      "718 [D loss: 0.712113, acc.: 62.50%] [G loss: 1.147680]\n",
      "719 [D loss: 0.667014, acc.: 59.38%] [G loss: 1.155566]\n",
      "720 [D loss: 0.749196, acc.: 53.12%] [G loss: 0.890293]\n",
      "721 [D loss: 0.801305, acc.: 53.12%] [G loss: 0.892440]\n",
      "722 [D loss: 0.677199, acc.: 62.50%] [G loss: 1.180700]\n",
      "723 [D loss: 0.658760, acc.: 64.06%] [G loss: 1.195973]\n",
      "724 [D loss: 0.677325, acc.: 56.25%] [G loss: 1.154743]\n",
      "725 [D loss: 0.628482, acc.: 59.38%] [G loss: 1.069184]\n",
      "726 [D loss: 0.725174, acc.: 50.00%] [G loss: 1.159866]\n",
      "727 [D loss: 0.670305, acc.: 59.38%] [G loss: 0.859770]\n",
      "728 [D loss: 0.684133, acc.: 56.25%] [G loss: 1.271417]\n",
      "729 [D loss: 0.694308, acc.: 53.12%] [G loss: 1.147028]\n",
      "730 [D loss: 0.576648, acc.: 70.31%] [G loss: 1.159837]\n",
      "731 [D loss: 0.768772, acc.: 56.25%] [G loss: 1.075757]\n",
      "732 [D loss: 0.668267, acc.: 64.06%] [G loss: 1.254841]\n",
      "733 [D loss: 0.761916, acc.: 48.44%] [G loss: 1.155281]\n",
      "734 [D loss: 0.672441, acc.: 62.50%] [G loss: 1.340433]\n",
      "735 [D loss: 0.566009, acc.: 68.75%] [G loss: 0.972823]\n",
      "736 [D loss: 0.617113, acc.: 67.19%] [G loss: 1.081410]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "737 [D loss: 0.633131, acc.: 68.75%] [G loss: 1.186728]\n",
      "738 [D loss: 0.617721, acc.: 62.50%] [G loss: 1.134958]\n",
      "739 [D loss: 0.579143, acc.: 71.88%] [G loss: 1.080682]\n",
      "740 [D loss: 0.661138, acc.: 57.81%] [G loss: 0.970509]\n",
      "741 [D loss: 0.516466, acc.: 79.69%] [G loss: 1.391673]\n",
      "742 [D loss: 0.753032, acc.: 53.12%] [G loss: 1.089189]\n",
      "743 [D loss: 0.571276, acc.: 75.00%] [G loss: 1.109904]\n",
      "744 [D loss: 0.637394, acc.: 65.62%] [G loss: 1.302953]\n",
      "745 [D loss: 0.554059, acc.: 75.00%] [G loss: 1.311953]\n",
      "746 [D loss: 0.588248, acc.: 67.19%] [G loss: 1.138351]\n",
      "747 [D loss: 0.686019, acc.: 54.69%] [G loss: 1.324281]\n",
      "748 [D loss: 0.822216, acc.: 45.31%] [G loss: 1.259769]\n",
      "749 [D loss: 0.743875, acc.: 57.81%] [G loss: 1.355719]\n",
      "750 [D loss: 0.804996, acc.: 53.12%] [G loss: 1.041138]\n",
      "751 [D loss: 0.833409, acc.: 45.31%] [G loss: 1.248653]\n",
      "752 [D loss: 0.636387, acc.: 64.06%] [G loss: 1.155897]\n",
      "753 [D loss: 0.683590, acc.: 62.50%] [G loss: 1.580840]\n",
      "754 [D loss: 0.817666, acc.: 50.00%] [G loss: 1.078312]\n",
      "755 [D loss: 0.561860, acc.: 64.06%] [G loss: 1.148717]\n",
      "756 [D loss: 0.631970, acc.: 56.25%] [G loss: 1.147390]\n",
      "757 [D loss: 0.630674, acc.: 65.62%] [G loss: 1.122672]\n",
      "758 [D loss: 0.757308, acc.: 43.75%] [G loss: 1.063741]\n",
      "759 [D loss: 0.820020, acc.: 48.44%] [G loss: 1.062637]\n",
      "760 [D loss: 0.651406, acc.: 51.56%] [G loss: 1.239570]\n",
      "761 [D loss: 0.714237, acc.: 53.12%] [G loss: 1.062215]\n",
      "762 [D loss: 0.689960, acc.: 60.94%] [G loss: 1.415606]\n",
      "763 [D loss: 0.611106, acc.: 65.62%] [G loss: 1.045486]\n",
      "764 [D loss: 0.715863, acc.: 54.69%] [G loss: 1.019341]\n",
      "765 [D loss: 0.667129, acc.: 60.94%] [G loss: 1.246064]\n",
      "766 [D loss: 0.587884, acc.: 67.19%] [G loss: 1.311041]\n",
      "767 [D loss: 0.746887, acc.: 45.31%] [G loss: 0.940573]\n",
      "768 [D loss: 0.678356, acc.: 60.94%] [G loss: 1.136971]\n",
      "769 [D loss: 0.625970, acc.: 64.06%] [G loss: 1.178113]\n",
      "770 [D loss: 0.647665, acc.: 67.19%] [G loss: 1.398700]\n",
      "771 [D loss: 0.545042, acc.: 75.00%] [G loss: 1.408131]\n",
      "772 [D loss: 0.639625, acc.: 67.19%] [G loss: 0.902725]\n",
      "773 [D loss: 0.656765, acc.: 59.38%] [G loss: 1.339970]\n",
      "774 [D loss: 0.662009, acc.: 57.81%] [G loss: 1.313872]\n",
      "775 [D loss: 0.622236, acc.: 64.06%] [G loss: 1.441356]\n",
      "776 [D loss: 0.740018, acc.: 50.00%] [G loss: 1.189280]\n",
      "777 [D loss: 0.668632, acc.: 59.38%] [G loss: 1.114779]\n",
      "778 [D loss: 0.556660, acc.: 67.19%] [G loss: 1.170515]\n",
      "779 [D loss: 0.755164, acc.: 57.81%] [G loss: 1.273625]\n",
      "780 [D loss: 0.691736, acc.: 57.81%] [G loss: 1.211601]\n",
      "781 [D loss: 0.611728, acc.: 67.19%] [G loss: 1.285015]\n",
      "782 [D loss: 0.643079, acc.: 59.38%] [G loss: 1.316364]\n",
      "783 [D loss: 0.671698, acc.: 59.38%] [G loss: 1.189159]\n",
      "784 [D loss: 0.827744, acc.: 57.81%] [G loss: 1.243244]\n",
      "785 [D loss: 0.656594, acc.: 56.25%] [G loss: 1.327020]\n",
      "786 [D loss: 0.760537, acc.: 54.69%] [G loss: 1.215018]\n",
      "787 [D loss: 0.684735, acc.: 59.38%] [G loss: 1.192368]\n",
      "788 [D loss: 0.658778, acc.: 60.94%] [G loss: 1.120089]\n",
      "789 [D loss: 0.635083, acc.: 64.06%] [G loss: 1.176138]\n",
      "790 [D loss: 0.648797, acc.: 54.69%] [G loss: 1.052601]\n",
      "791 [D loss: 0.604387, acc.: 60.94%] [G loss: 1.372865]\n",
      "792 [D loss: 0.636271, acc.: 65.62%] [G loss: 1.236249]\n",
      "793 [D loss: 0.708786, acc.: 48.44%] [G loss: 1.251165]\n",
      "794 [D loss: 0.711387, acc.: 62.50%] [G loss: 1.498261]\n",
      "795 [D loss: 0.648067, acc.: 65.62%] [G loss: 1.439930]\n",
      "796 [D loss: 0.618154, acc.: 73.44%] [G loss: 1.268547]\n",
      "797 [D loss: 0.838157, acc.: 40.62%] [G loss: 0.897674]\n",
      "798 [D loss: 0.667055, acc.: 57.81%] [G loss: 1.180820]\n",
      "799 [D loss: 0.598076, acc.: 67.19%] [G loss: 1.273878]\n",
      "800 [D loss: 0.743495, acc.: 53.12%] [G loss: 1.076355]\n",
      "801 [D loss: 0.672180, acc.: 64.06%] [G loss: 1.228020]\n",
      "802 [D loss: 0.708913, acc.: 56.25%] [G loss: 1.237838]\n",
      "803 [D loss: 0.772179, acc.: 45.31%] [G loss: 1.173383]\n",
      "804 [D loss: 0.679732, acc.: 65.62%] [G loss: 1.142538]\n",
      "805 [D loss: 0.751515, acc.: 54.69%] [G loss: 1.071151]\n",
      "806 [D loss: 0.768918, acc.: 50.00%] [G loss: 0.994574]\n",
      "807 [D loss: 0.718995, acc.: 48.44%] [G loss: 1.210689]\n",
      "808 [D loss: 0.639315, acc.: 60.94%] [G loss: 1.210877]\n",
      "809 [D loss: 0.603674, acc.: 67.19%] [G loss: 1.003138]\n",
      "810 [D loss: 0.700589, acc.: 62.50%] [G loss: 1.172260]\n",
      "811 [D loss: 0.655106, acc.: 57.81%] [G loss: 1.331526]\n",
      "812 [D loss: 0.671204, acc.: 62.50%] [G loss: 1.207205]\n",
      "813 [D loss: 0.824648, acc.: 46.88%] [G loss: 1.166967]\n",
      "814 [D loss: 0.746954, acc.: 56.25%] [G loss: 0.998895]\n",
      "815 [D loss: 0.858094, acc.: 35.94%] [G loss: 1.206087]\n",
      "816 [D loss: 0.710294, acc.: 51.56%] [G loss: 0.940538]\n",
      "817 [D loss: 0.665599, acc.: 64.06%] [G loss: 1.137926]\n",
      "818 [D loss: 0.624482, acc.: 68.75%] [G loss: 0.992188]\n",
      "819 [D loss: 0.714716, acc.: 46.88%] [G loss: 1.136559]\n",
      "820 [D loss: 0.624747, acc.: 59.38%] [G loss: 1.320874]\n",
      "821 [D loss: 0.628490, acc.: 64.06%] [G loss: 1.243066]\n",
      "822 [D loss: 0.628116, acc.: 67.19%] [G loss: 1.144077]\n",
      "823 [D loss: 0.683269, acc.: 53.12%] [G loss: 0.897759]\n",
      "824 [D loss: 0.716291, acc.: 54.69%] [G loss: 0.916641]\n",
      "825 [D loss: 0.735632, acc.: 54.69%] [G loss: 0.904260]\n",
      "826 [D loss: 0.725540, acc.: 53.12%] [G loss: 1.026922]\n",
      "827 [D loss: 0.563773, acc.: 71.88%] [G loss: 1.231832]\n",
      "828 [D loss: 0.677642, acc.: 57.81%] [G loss: 1.279944]\n",
      "829 [D loss: 0.774352, acc.: 48.44%] [G loss: 1.135973]\n",
      "830 [D loss: 0.714073, acc.: 57.81%] [G loss: 1.117497]\n",
      "831 [D loss: 0.724733, acc.: 59.38%] [G loss: 0.997251]\n",
      "832 [D loss: 0.739288, acc.: 62.50%] [G loss: 1.101606]\n",
      "833 [D loss: 0.538068, acc.: 73.44%] [G loss: 1.140953]\n",
      "834 [D loss: 0.697014, acc.: 54.69%] [G loss: 1.036372]\n",
      "835 [D loss: 0.771231, acc.: 53.12%] [G loss: 1.260055]\n",
      "836 [D loss: 0.814654, acc.: 54.69%] [G loss: 1.095743]\n",
      "837 [D loss: 0.678773, acc.: 57.81%] [G loss: 0.991943]\n",
      "838 [D loss: 0.801858, acc.: 53.12%] [G loss: 1.099925]\n",
      "839 [D loss: 0.740223, acc.: 56.25%] [G loss: 0.997931]\n",
      "840 [D loss: 0.709794, acc.: 59.38%] [G loss: 1.073208]\n",
      "841 [D loss: 0.802996, acc.: 45.31%] [G loss: 0.940308]\n",
      "842 [D loss: 0.699983, acc.: 60.94%] [G loss: 1.177031]\n",
      "843 [D loss: 0.769861, acc.: 57.81%] [G loss: 1.127974]\n",
      "844 [D loss: 0.758495, acc.: 60.94%] [G loss: 1.024687]\n",
      "845 [D loss: 0.576961, acc.: 71.88%] [G loss: 1.158341]\n",
      "846 [D loss: 0.780094, acc.: 51.56%] [G loss: 1.077923]\n",
      "847 [D loss: 0.657176, acc.: 67.19%] [G loss: 1.084186]\n",
      "848 [D loss: 0.744540, acc.: 50.00%] [G loss: 0.991778]\n",
      "849 [D loss: 0.730267, acc.: 54.69%] [G loss: 1.174816]\n",
      "850 [D loss: 0.597939, acc.: 62.50%] [G loss: 1.288278]\n",
      "851 [D loss: 0.571220, acc.: 73.44%] [G loss: 1.089043]\n",
      "852 [D loss: 0.550449, acc.: 78.12%] [G loss: 1.347297]\n",
      "853 [D loss: 0.579922, acc.: 75.00%] [G loss: 1.373601]\n",
      "854 [D loss: 0.684272, acc.: 57.81%] [G loss: 1.189764]\n",
      "855 [D loss: 0.632731, acc.: 65.62%] [G loss: 1.413589]\n",
      "856 [D loss: 0.775021, acc.: 51.56%] [G loss: 1.169619]\n",
      "857 [D loss: 0.718268, acc.: 51.56%] [G loss: 1.203106]\n",
      "858 [D loss: 0.664786, acc.: 62.50%] [G loss: 1.377003]\n",
      "859 [D loss: 0.690026, acc.: 54.69%] [G loss: 1.065265]\n",
      "860 [D loss: 0.629814, acc.: 70.31%] [G loss: 1.284418]\n",
      "861 [D loss: 0.671296, acc.: 62.50%] [G loss: 1.103006]\n",
      "862 [D loss: 0.798752, acc.: 46.88%] [G loss: 1.119954]\n",
      "863 [D loss: 0.695460, acc.: 62.50%] [G loss: 1.037893]\n",
      "864 [D loss: 0.732515, acc.: 50.00%] [G loss: 0.869518]\n",
      "865 [D loss: 0.722131, acc.: 56.25%] [G loss: 1.111086]\n",
      "866 [D loss: 0.721045, acc.: 65.62%] [G loss: 1.159209]\n",
      "867 [D loss: 0.609746, acc.: 64.06%] [G loss: 1.240074]\n",
      "868 [D loss: 0.611752, acc.: 67.19%] [G loss: 1.170893]\n",
      "869 [D loss: 0.623935, acc.: 67.19%] [G loss: 1.090867]\n",
      "870 [D loss: 0.684151, acc.: 59.38%] [G loss: 1.046796]\n",
      "871 [D loss: 0.704842, acc.: 56.25%] [G loss: 0.952759]\n",
      "872 [D loss: 0.712672, acc.: 60.94%] [G loss: 1.124604]\n",
      "873 [D loss: 0.596751, acc.: 67.19%] [G loss: 1.028785]\n",
      "874 [D loss: 0.742318, acc.: 51.56%] [G loss: 1.084018]\n",
      "875 [D loss: 0.760571, acc.: 53.12%] [G loss: 1.220031]\n",
      "876 [D loss: 0.662036, acc.: 64.06%] [G loss: 1.312214]\n",
      "877 [D loss: 0.685722, acc.: 62.50%] [G loss: 1.109680]\n",
      "878 [D loss: 0.586371, acc.: 75.00%] [G loss: 1.166270]\n",
      "879 [D loss: 0.721264, acc.: 54.69%] [G loss: 1.059777]\n",
      "880 [D loss: 0.739661, acc.: 45.31%] [G loss: 1.088788]\n",
      "881 [D loss: 0.779688, acc.: 42.19%] [G loss: 1.128599]\n",
      "882 [D loss: 0.759091, acc.: 43.75%] [G loss: 1.348243]\n",
      "883 [D loss: 0.614125, acc.: 65.62%] [G loss: 1.136865]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "884 [D loss: 0.528648, acc.: 71.88%] [G loss: 1.373717]\n",
      "885 [D loss: 0.624374, acc.: 68.75%] [G loss: 1.133422]\n",
      "886 [D loss: 0.815212, acc.: 53.12%] [G loss: 1.237067]\n",
      "887 [D loss: 0.706947, acc.: 53.12%] [G loss: 1.143337]\n",
      "888 [D loss: 0.672889, acc.: 53.12%] [G loss: 1.199716]\n",
      "889 [D loss: 0.702273, acc.: 60.94%] [G loss: 1.009615]\n",
      "890 [D loss: 0.766999, acc.: 51.56%] [G loss: 1.059567]\n",
      "891 [D loss: 0.735122, acc.: 57.81%] [G loss: 1.159432]\n",
      "892 [D loss: 0.553001, acc.: 68.75%] [G loss: 1.313669]\n",
      "893 [D loss: 0.805209, acc.: 48.44%] [G loss: 1.052771]\n",
      "894 [D loss: 0.757396, acc.: 54.69%] [G loss: 0.939153]\n",
      "895 [D loss: 0.591362, acc.: 64.06%] [G loss: 1.134081]\n",
      "896 [D loss: 0.619575, acc.: 65.62%] [G loss: 1.298447]\n",
      "897 [D loss: 0.720061, acc.: 51.56%] [G loss: 1.176075]\n",
      "898 [D loss: 0.707635, acc.: 50.00%] [G loss: 1.326139]\n",
      "899 [D loss: 0.664306, acc.: 56.25%] [G loss: 1.081971]\n",
      "900 [D loss: 0.636299, acc.: 62.50%] [G loss: 1.132024]\n",
      "901 [D loss: 0.843465, acc.: 48.44%] [G loss: 1.024699]\n",
      "902 [D loss: 0.842418, acc.: 39.06%] [G loss: 1.032346]\n",
      "903 [D loss: 0.652349, acc.: 67.19%] [G loss: 1.198280]\n",
      "904 [D loss: 0.685673, acc.: 60.94%] [G loss: 1.254343]\n",
      "905 [D loss: 0.659474, acc.: 57.81%] [G loss: 0.874460]\n",
      "906 [D loss: 0.704558, acc.: 60.94%] [G loss: 0.950600]\n",
      "907 [D loss: 0.637398, acc.: 60.94%] [G loss: 1.056596]\n",
      "908 [D loss: 0.679797, acc.: 54.69%] [G loss: 1.119164]\n",
      "909 [D loss: 0.740270, acc.: 54.69%] [G loss: 1.286887]\n",
      "910 [D loss: 0.617466, acc.: 71.88%] [G loss: 1.210210]\n",
      "911 [D loss: 0.793690, acc.: 51.56%] [G loss: 1.114307]\n",
      "912 [D loss: 0.726902, acc.: 57.81%] [G loss: 1.314711]\n",
      "913 [D loss: 0.702604, acc.: 50.00%] [G loss: 1.196725]\n",
      "914 [D loss: 0.797291, acc.: 48.44%] [G loss: 0.945626]\n",
      "915 [D loss: 0.636308, acc.: 67.19%] [G loss: 1.188442]\n",
      "916 [D loss: 0.824186, acc.: 46.88%] [G loss: 1.009450]\n",
      "917 [D loss: 0.598498, acc.: 65.62%] [G loss: 1.185668]\n",
      "918 [D loss: 0.674238, acc.: 62.50%] [G loss: 1.178053]\n",
      "919 [D loss: 0.734846, acc.: 56.25%] [G loss: 1.131770]\n",
      "920 [D loss: 0.559721, acc.: 75.00%] [G loss: 1.352311]\n",
      "921 [D loss: 0.648792, acc.: 57.81%] [G loss: 1.359823]\n",
      "922 [D loss: 0.600127, acc.: 67.19%] [G loss: 1.102504]\n",
      "923 [D loss: 0.712424, acc.: 53.12%] [G loss: 1.282488]\n",
      "924 [D loss: 0.595499, acc.: 76.56%] [G loss: 1.006784]\n",
      "925 [D loss: 0.702626, acc.: 57.81%] [G loss: 1.137597]\n",
      "926 [D loss: 0.727867, acc.: 53.12%] [G loss: 1.241195]\n",
      "927 [D loss: 0.695856, acc.: 60.94%] [G loss: 1.050752]\n",
      "928 [D loss: 0.667258, acc.: 62.50%] [G loss: 1.211738]\n",
      "929 [D loss: 0.676796, acc.: 54.69%] [G loss: 1.153368]\n",
      "930 [D loss: 0.636878, acc.: 62.50%] [G loss: 1.339756]\n",
      "931 [D loss: 0.802808, acc.: 45.31%] [G loss: 1.036791]\n",
      "932 [D loss: 0.662786, acc.: 56.25%] [G loss: 0.939348]\n",
      "933 [D loss: 0.653847, acc.: 64.06%] [G loss: 1.135601]\n",
      "934 [D loss: 0.659846, acc.: 57.81%] [G loss: 1.247657]\n",
      "935 [D loss: 0.727962, acc.: 53.12%] [G loss: 1.109907]\n",
      "936 [D loss: 0.663371, acc.: 60.94%] [G loss: 1.206587]\n",
      "937 [D loss: 0.649307, acc.: 62.50%] [G loss: 1.325440]\n",
      "938 [D loss: 0.799181, acc.: 48.44%] [G loss: 0.996709]\n",
      "939 [D loss: 0.655886, acc.: 65.62%] [G loss: 1.370470]\n",
      "940 [D loss: 0.684947, acc.: 57.81%] [G loss: 1.045030]\n",
      "941 [D loss: 0.667387, acc.: 60.94%] [G loss: 1.277142]\n",
      "942 [D loss: 0.617175, acc.: 64.06%] [G loss: 1.738005]\n",
      "943 [D loss: 0.676382, acc.: 60.94%] [G loss: 1.307584]\n",
      "944 [D loss: 0.703582, acc.: 50.00%] [G loss: 1.037609]\n",
      "945 [D loss: 0.769296, acc.: 48.44%] [G loss: 1.366307]\n",
      "946 [D loss: 0.627406, acc.: 65.62%] [G loss: 1.366818]\n",
      "947 [D loss: 0.654748, acc.: 57.81%] [G loss: 1.148189]\n",
      "948 [D loss: 0.611421, acc.: 68.75%] [G loss: 1.231808]\n",
      "949 [D loss: 0.703915, acc.: 51.56%] [G loss: 1.044056]\n",
      "950 [D loss: 0.690246, acc.: 56.25%] [G loss: 1.161470]\n",
      "951 [D loss: 0.693772, acc.: 53.12%] [G loss: 1.243524]\n",
      "952 [D loss: 0.689362, acc.: 54.69%] [G loss: 1.064462]\n",
      "953 [D loss: 0.592301, acc.: 65.62%] [G loss: 0.911416]\n",
      "954 [D loss: 0.712984, acc.: 59.38%] [G loss: 1.009676]\n",
      "955 [D loss: 0.834379, acc.: 51.56%] [G loss: 0.942979]\n",
      "956 [D loss: 0.631042, acc.: 64.06%] [G loss: 1.101922]\n",
      "957 [D loss: 0.704507, acc.: 54.69%] [G loss: 0.999830]\n",
      "958 [D loss: 0.682603, acc.: 62.50%] [G loss: 0.978734]\n",
      "959 [D loss: 0.623583, acc.: 70.31%] [G loss: 1.185507]\n",
      "960 [D loss: 0.891836, acc.: 39.06%] [G loss: 0.902885]\n",
      "961 [D loss: 0.627672, acc.: 65.62%] [G loss: 1.055375]\n",
      "962 [D loss: 0.634504, acc.: 65.62%] [G loss: 1.170021]\n",
      "963 [D loss: 0.592547, acc.: 67.19%] [G loss: 1.205135]\n",
      "964 [D loss: 0.753305, acc.: 48.44%] [G loss: 1.011755]\n",
      "965 [D loss: 0.619353, acc.: 65.62%] [G loss: 1.036963]\n",
      "966 [D loss: 0.707236, acc.: 59.38%] [G loss: 0.932764]\n",
      "967 [D loss: 0.641028, acc.: 67.19%] [G loss: 1.262813]\n",
      "968 [D loss: 0.727674, acc.: 50.00%] [G loss: 1.221323]\n",
      "969 [D loss: 0.693325, acc.: 59.38%] [G loss: 0.869575]\n",
      "970 [D loss: 0.643304, acc.: 59.38%] [G loss: 1.193032]\n",
      "971 [D loss: 0.690880, acc.: 59.38%] [G loss: 0.956821]\n",
      "972 [D loss: 0.623801, acc.: 65.62%] [G loss: 1.246630]\n",
      "973 [D loss: 0.604225, acc.: 68.75%] [G loss: 1.035811]\n",
      "974 [D loss: 0.725588, acc.: 59.38%] [G loss: 1.164518]\n",
      "975 [D loss: 0.716217, acc.: 56.25%] [G loss: 1.089269]\n",
      "976 [D loss: 0.685346, acc.: 59.38%] [G loss: 0.868119]\n",
      "977 [D loss: 0.702249, acc.: 56.25%] [G loss: 1.046331]\n",
      "978 [D loss: 0.788526, acc.: 50.00%] [G loss: 0.982250]\n",
      "979 [D loss: 0.645367, acc.: 56.25%] [G loss: 0.873685]\n",
      "980 [D loss: 0.581541, acc.: 68.75%] [G loss: 1.043026]\n",
      "981 [D loss: 0.669378, acc.: 56.25%] [G loss: 1.274953]\n",
      "982 [D loss: 0.737723, acc.: 53.12%] [G loss: 1.077349]\n",
      "983 [D loss: 0.629213, acc.: 64.06%] [G loss: 1.069213]\n",
      "984 [D loss: 0.835202, acc.: 43.75%] [G loss: 1.050572]\n",
      "985 [D loss: 0.674434, acc.: 57.81%] [G loss: 1.263357]\n",
      "986 [D loss: 0.691805, acc.: 65.62%] [G loss: 1.118364]\n",
      "987 [D loss: 0.671248, acc.: 57.81%] [G loss: 1.092316]\n",
      "988 [D loss: 0.641827, acc.: 65.62%] [G loss: 1.011597]\n",
      "989 [D loss: 0.616703, acc.: 65.62%] [G loss: 1.189690]\n",
      "990 [D loss: 0.747162, acc.: 56.25%] [G loss: 1.231118]\n",
      "991 [D loss: 0.607215, acc.: 70.31%] [G loss: 1.169273]\n",
      "992 [D loss: 0.590649, acc.: 71.88%] [G loss: 1.054856]\n",
      "993 [D loss: 0.661841, acc.: 60.94%] [G loss: 1.077285]\n",
      "994 [D loss: 0.897814, acc.: 42.19%] [G loss: 0.786489]\n",
      "995 [D loss: 0.723607, acc.: 48.44%] [G loss: 0.845322]\n",
      "996 [D loss: 0.665992, acc.: 64.06%] [G loss: 1.176874]\n",
      "997 [D loss: 0.807837, acc.: 53.12%] [G loss: 1.217030]\n",
      "998 [D loss: 0.749650, acc.: 53.12%] [G loss: 0.960044]\n",
      "999 [D loss: 0.602176, acc.: 68.75%] [G loss: 0.938784]\n",
      "1000 [D loss: 0.709199, acc.: 60.94%] [G loss: 0.929537]\n"
     ]
    }
   ],
   "source": [
    "train(epochs=1001, batch_size=32, save_interval=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
