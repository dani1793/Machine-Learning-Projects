{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Gaussian Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with simple non-linear regression problem. We are provided with 2D data points in x and y, we would like to learn the pattern of x and y coordinates and predict the y values that are unknown. However, we would also like to know the error boundaries in which our prediction lie. From the statement we could imagine our predictor to be a curve with error boundaries defined on both sides.\n",
    "\n",
    "We could solve this non-linear regression with error bars using simple gaussian distribution. \n",
    "\n",
    "Reviewing the multivariant gaussian we know that the coovariance matrix shows the coovariance between the gaussians, 0 means that the gaussian are not coorelated at all where as 1 means that they are the same gaussians. The diagonals of coovariance matrix 1 and the matrix is symettric, where the non diagonal elements of matrix represents the coovariance between the two gaussians.\n",
    "\n",
    "Another concept that would be useful in construction of gaussian processes would be marginal distribution. suppose we marginally distribute the gaussian distribution. We again would get another gaussian distribution. We could use this property to predict the value of y2 given y1. It is important to note that in this case the mean of the distribution would be linearly propertional to the value of y1.  \n",
    "\n",
    "For 2D multi-variate gaussian we can easily visualize and sample the points, using contour representation. However, there is another way to visualize the data points. We have the gaussian components represented by x-axis and the value that a gaussian component could take is on y-axis. We can than plot the values for each gaussian component, as we sample along the multivariant gaussian we can see the covaraince effect it has on the points of two gaussians. Again we could marginalize the process by fixing the value of one of the gaussian component and sampling from the other component, the points would be bound by the corelation it have with the other gaussian.\n",
    "\n",
    "We could generalize this with more than two dimensional gaussians. There we can see the relationship between all the gaussins as we sample around the contours.\n",
    "\n",
    "Now we can generate a smooth curve that looks like a prediction we did for non-linear regression with error bars. The figure below shows the formulas to compute the covariance matrix for multi variant gaussians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/gaussian_process_summary.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K function is known as Coovariance function as it generates the covariance matrix. This funciton is than used to compute the coovariance matrix for the gaussian process. Using these functions we can generate the coovariance matrix for arbitrary number of x and hence can solve the non regression problem discussed in start.\n",
    "\n",
    "We can define multi variant gaussian process as multi variant gaussian with infinite mean vector and infinite by infinite covaraince matrix.\n",
    "\n",
    "There are two hyperparameters in the covariance function namely l and sigma. l is the horizontal scale, it represents how quickly or slowly the curve changes its shape. if the value of l is large there is smooth curve with slow variation between the gaussian functions, where as if the value of l is small the gaussian functions are quickly varaying. sigma on the other hand decides on the vertical scale, what is the range of values that the functions could take. These are the hyperparameters and are learnt by the data provided.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage and Limitation\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in theory section we need to marginalize the gaussian processes. To perform the marginalization given the joint distribution we use the Bayes rule. However to calculate the marginalized gaussian we would require to take inverse of the coovariance matrix. This poses a problem in calculation because as per the defination of gaussian processes coovarinace matrix could be infinite by infinite.\n",
    "\n",
    "The figure below shows the interpretation of gaussian process consisting of two gaussain functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/interpretation-of-marginalization.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure shows the predictive mean and the covariance and its expression. Here y1 is the training data where as y2 are your prediction data points (**Have to clear that, not clear yet!!!**). The predictive mean as mentioned before could be simplified and linearly related to the data. Predictive covariance can be intuitively divided into two parts namely prior uncertainty and reduction in uncertainty, logically as we see more data the redcution term increases making the uncertainity small. \n",
    "\n",
    "As we can see from the equation of covariance, it requires inverse of a matrix. To compute it in real time is not feasible as it would require n3 time\n",
    "\n",
    "**The more correlated the data points the more confident we are about our predictions and the less variation we see in our predicitons.**\n",
    "\n",
    "The figure below shows the effect different values of l have on the predictive curve. We can see that long l means that the data is high correlated which results in small error bars as we go further away from the data points. Whereas when l is short we see more wiggliness in the data and as we go further away from the data we get the prediction of 0 value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/hyper-parameter-l.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the optimal hyper parameters from the data we use the maximum likelihood w.r.t hyperparameters. The resultant likelihood would be multivariant gaussian and we can than compute the optimal hyper parameters using any gradient decent or optimization algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of covarinace functions\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different types of covariance functions and we can compose differnet covariance functions according to the data provided and the scenerio we are in. However there are few basic covariance function which are popular and they are as follows\n",
    "* **Brownian motion** (we take absolute difference between the data points): This gives us straight line between the data points (also known as brownian gaps)\n",
    "* **Rational Quadratic** (This have a power as hyper parameter that controls the shape of predicton curve)\n",
    "* **periodic covariance** (This type of covariance have cosine funtion multiplied to the covariance function): This gives periodic predictive curves\n",
    "\n",
    "The figure below shows the shape obtained from different type of covaraince functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/covarinace-types.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot use the Bayessian model comparison to model compare different gaussian processes as it is highly dependent upon the prior. The comparison also require a lot of approximations and is not computation friendly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Gaussian Process to large data sets\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we generalize the gaussian process on problems which are non gaussian in nature like classification and regression with non gaussian noise we face two major problems; namely intractability and heavy computation due to inverse of the covaraince matrices.\n",
    "\n",
    "One of the methods for the large data sets prediction is **approximation of sparse dataset using EP (Expectation Propagation)**.\n",
    "\n",
    "The EP algorithm consist of 4 steps.\n",
    "- In first step we take off a psuedo data points from the gaussian process approximation. This would create a cavity in the gaussian process\n",
    "- We than fill this cavity with the data point from original dataset\n",
    "- We take the KL divergence of the two gaussian processes in step 2 and the approximated posterior and minimize it w.r.t approximated posterior. \n",
    "- After the optimal approximation is found we update the likelihoods\n",
    "\n",
    "The figure below shows the working of the EP algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/ep-algorithm.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian processes vs deep neural networks\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In general we have a decision making task and we need to have a well calibrated predictive uncertainity. \n",
    "- In statistical theory we need to create a very rich and powerful model even if we have small data, and let the data decide how much of the model it needs to use\n",
    "- Neural networks in general provide terriblly callibrated uncertainity estimates\n",
    "- We prefer non parametric models over parametric models because in parametric models the capacity of our parameters determine the capacity of the model. Once the parameters are set in the training phase we do not change them in the testing phase. As contrast to this non parameteric models have infinite dimensions of parameters, that means they have infinite capacity to learn from the training data\n",
    "- Most of the real world applications have heirarchical structure for the data.\n",
    "\n",
    "\n",
    "When we design a deep belief network using gaussian processes, we need a function that would decide the shape of the gaussian process for each layer. This structure is known as **automatic kernel desing**. We can think of deep gaussian processes as the neural network with infinitly wide hidden layers (i.e: gaussian processes are non parametric and have infinite number of parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
