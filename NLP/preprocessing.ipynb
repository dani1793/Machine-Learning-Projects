{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint  # pretty-printer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import gensim;\n",
    "import os;\n",
    "import nltk.data;\n",
    "import time;\n",
    "\n",
    "\n",
    "# path where the combined folder is present\n",
    "TRAIN_DIR_PATH ='./NLP/train-corpus';\n",
    "\n",
    "class Text(object):\n",
    "    def __init__(self,dirname):\n",
    "        self.dirname = dirname;\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            filename, fileExtension = os.path.splitext(os.path.join(self.dirname,fname))\n",
    "            if fileExtension == '.txt':\n",
    "                for line in open(os.path.join(self.dirname,fname)):\n",
    "                    token = self.tokenize(line) \n",
    "                    yield token\n",
    "\n",
    "    # The method does the following in order\n",
    "    # 1- It convert the copra paragraph into array of sentences using punkt tokenizer\n",
    "    # 2- It then tokenize each line in paragraph into words and remove the stop words and the special characters\n",
    "    def tokenize(self,line):\n",
    "        specialCharacters = ['@','#',',','.','(',')','*',';'] # array of special characters used for prune out the tokens\n",
    "        sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        tknzr = TweetTokenizer()\n",
    "        tokenized = sent_detector.tokenize(line.strip());\n",
    "        wordList = [];\n",
    "        for line in tokenized:\n",
    "            wordList.extend(tknzr.tokenize(line));\n",
    "        tokenized = [word for word in wordList if word not in\n",
    "        stopwords.words('english') and word not in specialCharacters]\n",
    "        return tokenized        \n",
    "\n",
    "def createAndSaveModel(sg=1, size= 100, mincount= 5, window= 5,\n",
    "corpusPath= TRAIN_DIR_PATH, saveFilePath = './wordToVec/modelTemp'):\n",
    "    sentence = Text(corpusPath);\n",
    "    model = gensim.models.Word2Vec(sentence, sg=sg,size= size,\n",
    "    min_count=mincount, window =window, workers=4);\n",
    "    pprint('saving data');\n",
    "    model.save(saveFilePath);\n",
    "\n",
    "def recordElapsedTime(str):\n",
    "    file = open('TimeForModel-test.txt ','a') \n",
    "    file.writelines(str)  \n",
    "    file.close() \n",
    "    \n",
    "# different word2Vec configurations tested for data\n",
    "models = {\n",
    "    'modelOne': {\n",
    "        'sg': 1, \n",
    "        'size': 600, \n",
    "        'mincount': 5, \n",
    "        'window': 10,\n",
    "        'saveFilePath': './NLP/wordToVec/model-sg-1-size-600-window-10'\n",
    "    },\n",
    "        'modelTwo': {\n",
    "        'sg': 1, \n",
    "        'size': 100, \n",
    "        'mincount': 5, \n",
    "        'window': 5,\n",
    "        'saveFilePath': './NLP/wordToVec/model-sg-1-size-100'\n",
    "    },\n",
    "        'modelThree': {\n",
    "        'sg': 2, \n",
    "        'size': 100, \n",
    "        'mincount': 5, \n",
    "        'window': 5,\n",
    "        'saveFilePath': './NLP/wordToVec/model-sg-2-size-100'\n",
    "    },\n",
    "        'modelFour': {\n",
    "        'sg': 1, \n",
    "        'size': 100, \n",
    "        'mincount': 10, \n",
    "        'window': 5,\n",
    "        'saveFilePath': './NLP/wordToVec/model-sg-1-size-100-mincount-10'\n",
    "    },\n",
    "        'modelFive': {\n",
    "        'sg': 1, \n",
    "        'size': 200, \n",
    "        'mincount': 10, \n",
    "        'window': 5,\n",
    "        'saveFilePath': './NLP/wordToVec/model-sg-1-size-200-mincount-10'\n",
    "    },\n",
    "        'modelSix': {\n",
    "        'sg': 1, \n",
    "        'size': 200, \n",
    "        'mincount': 5, \n",
    "        'window': 5,\n",
    "        'saveFilePath': './NLP/wordToVec/model-sg-1-size-200-mincount-50'\n",
    "    },\n",
    "    \n",
    "}\n",
    "\n",
    "# Model selected for testing\n",
    "SELECTED_MODEL = 'modelOne'\n",
    "        \n",
    "start = time.clock(); \n",
    "# Tokenize the reviews and feed it to word2Vec. The model created by word2Vec is saved.\n",
    "createAndSaveModel(sg=models[SELECTED_MODEL]['sg'],size=models[SELECTED_MODEL]['size'], window = models[SELECTED_MODEL]['window'], \n",
    "                   saveFilePath =models[SELECTED_MODEL]['saveFilePath']);   \n",
    "end = time.clock() ;\n",
    "elapsed = end - start;\n",
    "recordElapsedTime('Time spent in %s is: %f \\n'% (SELECTED_MODEL, elapsed));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
