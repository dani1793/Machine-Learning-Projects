{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file should be used to convert the words in sentences into integer tokens that should correspond to words in the selected embedding model. When the tokenized sentences are ready, we should be able to feed them directly into the model and find the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint  # pretty-printer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import gensim;\n",
    "import os;\n",
    "import nltk.data;\n",
    "import time;\n",
    "import numpy as np;\n",
    "import matplotlib.pyplot as plt;\n",
    "from sklearn.manifold import TSNE;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The method does the following in order\n",
    "# 1- It convert the copra paragraph into array of sentences using punkt tokenizer\n",
    "# 2- It then tokenize each line in paragraph into words and remove the stop words and the special characters\n",
    "def tokenize(line):\n",
    "    specialCharacters = ['@','#',',','.','(',')','*',';'] # array of special characters used for prune out the tokens\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    tknzr = TweetTokenizer()\n",
    "    tokenized = sent_detector.tokenize(line.strip());\n",
    "    wordList = [];\n",
    "    for line in tokenized:\n",
    "        wordList.extend(tknzr.tokenize(line));\n",
    "    tokenized = [word for word in wordList if word not in\n",
    "    stopwords.words('english') and word not in specialCharacters]\n",
    "    return tokenized    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens2Index(tokenArr, model):\n",
    "    index = np.array([]);\n",
    "    for token in tokenArr:\n",
    "        try:\n",
    "            index =  np.append(index,model.wv.index2word.index(token));\n",
    "        except ValueError:\n",
    "            index = np.append(index, -1);\n",
    "    return index;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedModel = 'model-sg-1-size-600-window-10'\n",
    "modelLoad = gensim.models.Word2Vec.load('./wordToVec/%s'%(selectedModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning model: 1000 / 25001\r",
      "learning model: 2000 / 25001\r",
      "learning model: 3000 / 25001\r",
      "learning model: 4000 / 25001\r",
      "learning model: 5000 / 25001\r",
      "learning model: 6000 / 25001\r",
      "learning model: 7000 / 25001\r",
      "learning model: 8000 / 25001\r",
      "learning model: 9000 / 25001\r",
      "learning model: 10000 / 25001\r",
      "learning model: 11000 / 25001\r",
      "learning model: 12000 / 25001\r",
      "learning model: 13000 / 25001\r",
      "learning model: 14000 / 25001\r",
      "learning model: 15000 / 25001\r",
      "learning model: 16000 / 25001\r",
      "learning model: 17000 / 25001\r",
      "learning model: 18000 / 25001\r",
      "learning model: 19000 / 25001\r",
      "learning model: 20000 / 25001\r",
      "learning model: 21000 / 25001\r",
      "learning model: 22000 / 25001\r",
      "learning model: 23000 / 25001\r",
      "learning model: 24000 / 25001\r",
      "learning model: 25000 / 25001\r"
     ]
    }
   ],
   "source": [
    "dirName = './train-corpus';\n",
    "readFiles = 0\n",
    "allFiles = len([name for name in os.listdir(dirName)])\n",
    "for fname in os.listdir(dirName):\n",
    "    readFiles = readFiles + 1\n",
    "    if readFiles % 1000 == 0:\n",
    "        print('learning model: %d / %d'%(readFiles, allFiles), end=\"\\r\" )\n",
    "    filename, fileExtension = os.path.splitext(os.path.join(dirName,fname))\n",
    "    if fileExtension == '.txt':\n",
    "        for line in open(os.path.join(dirName,fname)):\n",
    "            token = tokenize(line) \n",
    "            tokenized.append(tokens2Index(token, modelLoad));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('tokenized', np.array(tokenized))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
